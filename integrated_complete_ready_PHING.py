# -*-coding:utf-8-*-

import time
import robomaster
from robomaster import robot, camera as r_camera, blaster as r_blaster
import numpy as np
import math
import json
import matplotlib.pyplot as plt
from collections import deque
import traceback
import statistics
import os
import cv2
import threading
import queue

# =============================================================================
# ===== CONFIGURATION & PARAMETERS ============================================
# =============================================================================
SPEED_ROTATE = 480

# --- Sharp Distance Sensor Configuration ---
LEFT_SHARP_SENSOR_ID = 1
LEFT_SHARP_SENSOR_PORT = 1
LEFT_TARGET_CM = 13.0

RIGHT_SHARP_SENSOR_ID = 2
RIGHT_SHARP_SENSOR_PORT = 1
RIGHT_TARGET_CM = 13.0

# --- IR Sensor Configuration ---
LEFT_IR_SENSOR_ID = 1
LEFT_IR_SENSOR_PORT = 2
RIGHT_IR_SENSOR_ID = 2
RIGHT_IR_SENSOR_PORT = 2

# --- Sharp Sensor Detection Thresholds ---
SHARP_WALL_THRESHOLD_CM = 60.0  # ‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏ú‡∏ô‡∏±‡∏á
SHARP_STDEV_THRESHOLD = 0.2     # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡∏ß‡πà‡∏á

# --- ToF Centering Configuration (from dude_kum.py) ---
TOF_ADJUST_SPEED = 0.1             # ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤/‡∏ñ‡∏≠‡∏¢‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á‡πÇ‡∏´‡∏ô‡∏î
TOF_CALIBRATION_SLOPE = 0.0894     # ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Calibrate
TOF_CALIBRATION_Y_INTERCEPT = 3.8409 # ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Calibrate

GRID = 3

# --- Logical state for the grid map (from map_suay.py) ---
CURRENT_POSITION = (1,0)  # (‡πÅ‡∏ñ‡∏ß, ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå) here
CURRENT_DIRECTION =  0  # 0:North, 1:East, 2:South, 3:West here
TARGET_DESTINATION =CURRENT_POSITION #(1, 0)#here

# --- Physical state for the robot ---
CURRENT_TARGET_YAW = 0.0
ROBOT_FACE = 1  # 1,3,5.. = X axis, 2,4,6.. = Y axis

# --- NEW: IMU Drift Compensation Parameters ---
IMU_COMPENSATION_START_NODE_COUNT = 7      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ä‡∏î‡πÄ‡∏ä‡∏¢
IMU_COMPENSATION_NODE_INTERVAL = 10      # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ä‡∏î‡πÄ‡∏ä‡∏¢‡∏ó‡∏∏‡∏Å‡πÜ N ‡πÇ‡∏´‡∏ô‡∏î
IMU_COMPENSATION_DEG_PER_INTERVAL = -2.0 # ‡∏Ñ‡πà‡∏≤‡∏≠‡∏á‡∏®‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡∏î‡πÄ‡∏ä‡∏¢ (‡∏•‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß‡∏Ç‡∏ß‡∏≤)
IMU_DRIFT_COMPENSATION_DEG = 0.0           # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡∏ä‡∏î‡πÄ‡∏ä‡∏¢‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

# --- Occupancy Grid Parameters ---
PROB_OCC_GIVEN_OCC = {'tof': 0.95, 'sharp': 0.90} # ‡πÄ‡∏û‡∏¥‡πà‡∏° 'sharp'
PROB_OCC_GIVEN_FREE = {'tof': 0.05, 'sharp': 0.10} # ‡πÄ‡∏û‡∏¥‡πà‡∏° 'sharp'

LOG_ODDS_OCC = {
    'tof': math.log(PROB_OCC_GIVEN_OCC['tof'] / (1 - PROB_OCC_GIVEN_OCC['tof'])),
    'sharp': math.log(PROB_OCC_GIVEN_OCC['sharp'] / (1 - PROB_OCC_GIVEN_OCC['sharp']))
}
LOG_ODDS_FREE = {
    'tof': math.log(PROB_OCC_GIVEN_FREE['tof'] / (1 - PROB_OCC_GIVEN_FREE['tof'])),
    'sharp': math.log(PROB_OCC_GIVEN_FREE['sharp'] / (1 - PROB_OCC_GIVEN_FREE['sharp']))
}

# --- Decision Thresholds ---
OCCUPANCY_THRESHOLD = 0.7
FREE_THRESHOLD = 0.3

# --- Visualization Configuration ---
MAP_FIGURE_SIZE = (10, 8)  # (width, height) ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

# --- NEW: Timestamp Logging ---
POSITION_LOG = []  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤

# --- NEW: Resume Function Variables ---
RESUME_MODE = False  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î resume ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
DATA_FOLDER = r"F:\Coder\Year2-1\Robot_Module\Assignment\dude\James_path"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå JSON

def save_all_data(occupancy_map):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Map, Timestamps, Objects) ‡∏•‡∏á JSON"""
    try:
        print("üíæ Saving map and timestamp data...")
        
        # 1. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° objects
        final_map_data = {'nodes': []}
        for r in range(occupancy_map.height):
            for c in range(occupancy_map.width):
                cell = occupancy_map.grid[r][c]
                cell_data = {
                    "coordinate": {"row": r, "col": c},
                    "probability": round(cell.get_node_probability(), 3),
                    "is_occupied": cell.is_node_occupied(),
                    "walls": {
                        "north": cell.walls['N'].is_occupied(),
                        "south": cell.walls['S'].is_occupied(),
                        "east": cell.walls['E'].is_occupied(),
                        "west": cell.walls['W'].is_occupied()
                    },
                    "wall_probabilities": {
                        "north": round(cell.walls['N'].get_probability(), 3),
                        "south": round(cell.walls['S'].get_probability(), 3),
                        "east": round(cell.walls['E'].get_probability(), 3),
                        "west": round(cell.walls['W'].get_probability(), 3)
                    },
                    "objects": cell.objects if hasattr(cell, 'objects') else []
                }
                final_map_data["nodes"].append(cell_data)

        map_file = os.path.join(DATA_FOLDER, "Mapping_Top.json")
        with open(map_file, "w") as f:
            json.dump(final_map_data, f, indent=2)
        print(f"‚úÖ Final Hybrid Belief Map (with objects) saved to {map_file}")
        
        # 2. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• timestamp ‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        timestamp_data = {
            "session_info": {
                "start_time": POSITION_LOG[0]["iso_timestamp"] if POSITION_LOG else "N/A",
                "end_time": POSITION_LOG[-1]["iso_timestamp"] if POSITION_LOG else "N/A",
                "total_positions_logged": len(POSITION_LOG),
                "grid_size": f"{occupancy_map.height}x{occupancy_map.width}",
                "target_destination": list(TARGET_DESTINATION),
                "interrupted": not RESUME_MODE
            },
            "position_log": POSITION_LOG
        }
        
        timestamp_file = os.path.join(DATA_FOLDER, "Robot_Position_Timestamps.json")
        with open(timestamp_file, "w") as f:
            json.dump(timestamp_data, f, indent=2)
        print(f"‚úÖ Robot position timestamps saved to {timestamp_file}")
        
        # 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ (‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å map)
        all_detected_objects = []
        for r in range(occupancy_map.height):
            for c in range(occupancy_map.width):
                cell = occupancy_map.grid[r][c]
                if hasattr(cell, 'objects') and cell.objects:
                    for obj in cell.objects:
                        obj_with_pos = obj.copy()
                        obj_with_pos['cell_position'] = {'row': r, 'col': c}
                        all_detected_objects.append(obj_with_pos)
        
        objects_data = {
            "session_info": {
                "total_objects_detected": len(all_detected_objects),
                "detection_timestamp": time.time(),
                "grid_size": f"{occupancy_map.height}x{occupancy_map.width}"
            },
            "detected_objects": all_detected_objects
        }
        
        objects_file = os.path.join(DATA_FOLDER, "Detected_Objects.json")
        with open(objects_file, "w") as f:
            json.dump(objects_data, f, indent=2)
        print(f"‚úÖ Detected objects saved to {objects_file} (Total: {len(all_detected_objects)} objects)")
        
        return True
    except Exception as save_error:
        print(f"‚ùå Error saving data: {save_error}")
        traceback.print_exc()
        return False

# --- CAMERA HEALTH SHARED STATE ---
last_frame_received_ts = 0.0  # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á (capture thread)

def camera_is_healthy(timeout=2.0) -> bool:
    """
    ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏ô‡∏µ‡πâ
    timeout: ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ (default 2.0 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.5)
    """
    try:
        # ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ global manager ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ï‡∏≠‡∏ô initialize
        if not manager.connected.is_set():
            return False
    except Exception:
        return False
    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏î‡πâ frame (last_frame_received_ts = 0) ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ healthy ‡∏´‡∏≤‡∏Å‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà
    if last_frame_received_ts == 0:
        return True
    return (time.time() - last_frame_received_ts) <= timeout

def wait_for_camera_recovery(pause_label="Runtime", max_wait=120.0):
    """‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏∏‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô 30s ‡∏à‡∏∞‡∏™‡∏±‡πà‡∏á reconnect ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏≠‡∏ï‡πà‡∏≠"""
    print(f"üõë {pause_label}: Camera unhealthy ‚Üí locking chassis and waiting...")
    try:
        movement_controller.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
    except Exception:
        pass
    
    recovery_start = time.time()
    wait_start = time.time()
    reconnect_count = 0
    
    while not camera_is_healthy():
        elapsed = time.time() - recovery_start
        if elapsed > max_wait:
            print(f"‚ùå {pause_label}: Camera recovery failed after {max_wait}s. Giving up.")
            return False
            
        if time.time() - wait_start > 30.0:
            reconnect_count += 1
            print(f"‚ö†Ô∏è {pause_label}: Camera recovery timeout (30s). Forcing reconnect #{reconnect_count}...")
            try:
                manager.drop_and_reconnect()
            except Exception as e:
                print(f"‚ö†Ô∏è Reconnect error: {e}")
            wait_start = time.time()
            time.sleep(2.0)  # ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÉ‡∏´‡∏°‡πà
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏≠‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        if int(elapsed) % 5 == 0 and elapsed > 0:
            print(f"‚è≥ {pause_label}: Waiting for camera... ({elapsed:.0f}s elapsed)")
        
        time.sleep(0.5)
    
    print(f"‚úÖ {pause_label}: Camera recovered after {time.time() - recovery_start:.1f}s. Resuming...")
    return True

# =============================================================================
# ===== OBJECT DETECTION CONFIGURATION =======================================
# =============================================================================

# Object Detection Parameters
TARGET_SHAPE = "Circle"
TARGET_COLOR = "Red"

# PID Parameters
PID_KP = -0.25
PID_KI = -0.01
PID_KD = -0.03
DERIV_LPF_ALPHA = 0.25

MAX_YAW_SPEED = 220
MAX_PITCH_SPEED = 180
I_CLAMP = 2000.0

PIX_ERR_DEADZONE = 6
LOCK_TOL_X = 8
LOCK_TOL_Y = 8
LOCK_STABLE_COUNT = 6

# Camera Configuration
FRAME_W, FRAME_H = 960, 540
VERTICAL_FOV_DEG = 54.0
PIXELS_PER_DEG_V = FRAME_H / VERTICAL_FOV_DEG

PITCH_BIAS_DEG = 2.0
PITCH_BIAS_PIX = +PITCH_BIAS_DEG * PIXELS_PER_DEG_V

# ROI Configuration
ROI_Y0, ROI_H0, ROI_X0, ROI_W0 = 264, 270, 10, 911
ROI_SHIFT_PER_DEG = 6.0
ROI_Y_MIN, ROI_Y_MAX = 0, FRAME_H - 10

# GPU Configuration
USE_GPU = False
try:
    if cv2.cuda.getCudaEnabledDeviceCount() > 0:
        print("‚úÖ CUDA available, enabling GPU path")
        USE_GPU = True
    else:
        print("‚ö†Ô∏è CUDA not available, CPU path")
except Exception:
    print("‚ö†Ô∏è Skip CUDA check, CPU path")

# =============================================================================
# ===== SHARED VARIABLES & THREADING =========================================
# =============================================================================

# Object Detection Threading
frame_queue = queue.Queue(maxsize=1)
processed_output = {"details": []}
output_lock = threading.Lock()
stop_event = threading.Event()

# Gimbal angles
gimbal_angle_lock = threading.Lock()
gimbal_angles = (0.0, 0.0, 0.0, 0.0)

# Detection mode control
is_detecting_flag = {"v": False}  # Start with detection OFF
detection_timer = None
detection_start_time = None

# Object storage for map integration
detected_objects = []
object_lock = threading.Lock()

def sub_angle_cb(angle_info):
    global gimbal_angles
    with gimbal_angle_lock:
        gimbal_angles = tuple(angle_info)

# =============================================================================
# ===== HELPER FUNCTIONS ======================================================
# =============================================================================
def convert_adc_to_cm(adc_value):
    """Converts ADC value from Sharp sensor to centimeters."""
    if adc_value <= 0: return float('inf')
    # This formula is specific to the GP2Y0A21YK0F sensor.
    # You may need to re-calibrate for your specific sensor.
    return 30263 * (adc_value ** -1.352)

def calibrate_tof_value(raw_tof_value):
    """
    NEW: Converts raw ToF value (mm) to a calibrated distance in cm.
    From dude_kum.py.
    """
    try:
        if raw_tof_value is None or raw_tof_value <= 0:
            return float('inf')
        # The formula is: calibrated_cm = (slope * raw_mm) + y_intercept
        return (TOF_CALIBRATION_SLOPE * raw_tof_value) + TOF_CALIBRATION_Y_INTERCEPT
    except Exception:
        return float('inf')

def get_compensated_target_yaw():
    """
    NEW: Returns the current target yaw with the calculated IMU drift compensation.
    This function is now the single source of truth for the robot's target heading.
    """
    return CURRENT_TARGET_YAW + IMU_DRIFT_COMPENSATION_DEG

def log_position_timestamp(position, direction, action="arrived"):
    """
    NEW: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å timestamp ‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå
    """
    global POSITION_LOG
    timestamp = time.time()
    direction_names = ['North', 'East', 'South', 'West']
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á ISO timestamp
    dt = time.gmtime(timestamp)
    microseconds = int((timestamp % 1) * 1000000)
    iso_timestamp = time.strftime("%Y-%m-%dT%H:%M:%S", dt) + f".{microseconds:06d}Z"
    
    log_entry = {
        "timestamp": timestamp,
        "iso_timestamp": iso_timestamp,
        "position": list(position),
        "direction": direction_names[direction],
        "action": action,
        "yaw_angle": CURRENT_TARGET_YAW,
        "imu_compensation": IMU_DRIFT_COMPENSATION_DEG
    }
    
    POSITION_LOG.append(log_entry)
    print(f"üìç [{action}] Position: {position}, Direction: {direction_names[direction]}, Time: {log_entry['iso_timestamp']}")

def check_for_resume_data():
    """
    NEW: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö resume ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    map_file = os.path.join(DATA_FOLDER, "Mapping_Top.json")
    timestamp_file = os.path.join(DATA_FOLDER, "Robot_Position_Timestamps.json")
    
    if os.path.exists(map_file) and os.path.exists(timestamp_file):
        return True
    return False

def load_resume_data():
    """
    NEW: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠ resume ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    """
    global CURRENT_POSITION, CURRENT_DIRECTION, CURRENT_TARGET_YAW, ROBOT_FACE, IMU_DRIFT_COMPENSATION_DEG, POSITION_LOG, RESUME_MODE
    
    try:
        print("üîÑ Loading resume data...")
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• timestamp
        timestamp_file = os.path.join(DATA_FOLDER, "Robot_Position_Timestamps.json")
        with open(timestamp_file, "r", encoding="utf-8") as f:
            timestamp_data = json.load(f)
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ global variables ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• timestamp
        if timestamp_data["position_log"]:
            last_log = timestamp_data["position_log"][-1]
            CURRENT_POSITION = tuple(last_log["position"])
            CURRENT_TARGET_YAW = last_log["yaw_angle"]
            IMU_DRIFT_COMPENSATION_DEG = last_log["imu_compensation"]
            POSITION_LOG = timestamp_data["position_log"]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì direction ‡∏à‡∏≤‡∏Å yaw angle
            yaw = last_log["yaw_angle"]
            if -45 <= yaw <= 45:
                CURRENT_DIRECTION = 0  # North
                ROBOT_FACE = 1
            elif 45 < yaw <= 135:
                CURRENT_DIRECTION = 1  # East
                ROBOT_FACE = 2
            elif 135 < yaw or yaw <= -135:
                CURRENT_DIRECTION = 2  # South
                ROBOT_FACE = 3
            else:
                CURRENT_DIRECTION = 3  # West
                ROBOT_FACE = 4
        
        print(f"‚úÖ Resume data loaded:")
        print(f"   Position: {CURRENT_POSITION}")
        print(f"   Direction: {['North', 'East', 'South', 'West'][CURRENT_DIRECTION]}")
        print(f"   Yaw: {CURRENT_TARGET_YAW:.1f}¬∞")
        print(f"   IMU Compensation: {IMU_DRIFT_COMPENSATION_DEG:.1f}¬∞")
        print(f"   Previous positions logged: {len(POSITION_LOG)}")
        
        RESUME_MODE = True
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading resume data: {e}")
        return False

def create_occupancy_map_from_json():
    """
    NEW: ‡∏™‡∏£‡πâ‡∏≤‡∏á OccupancyGridMap ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON
    """
    try:
        map_file = os.path.join(DATA_FOLDER, "Mapping_Top.json")
        with open(map_file, "r", encoding="utf-8") as f:
            map_data = json.load(f)
        
        # ‡∏´‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏¥‡∏î
        max_row = max(node['coordinate']['row'] for node in map_data['nodes'])
        max_col = max(node['coordinate']['col'] for node in map_data['nodes'])
        width = max_col + 1
        height = max_row + 1
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á OccupancyGridMap
        occupancy_map = OccupancyGridMap(width, height)
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å JSON ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô occupancy_map
        for node_data in map_data['nodes']:
            r = node_data['coordinate']['row']
            c = node_data['coordinate']['col']
            cell = occupancy_map.grid[r][c]
            
            # ‡πÇ‡∏´‡∏•‡∏î node probability
            cell.log_odds_occupied = math.log(node_data['probability'] / (1 - node_data['probability'])) if node_data['probability'] != 0.5 else 0
            
            # ‡πÇ‡∏´‡∏•‡∏î wall probabilities
            walls = node_data['wall_probabilities']
            for direction, prob in walls.items():
                if direction == 'north':
                    cell.walls['N'].log_odds = math.log(prob / (1 - prob)) if prob != 0.5 else 0
                elif direction == 'south':
                    cell.walls['S'].log_odds = math.log(prob / (1 - prob)) if prob != 0.5 else 0
                elif direction == 'east':
                    cell.walls['E'].log_odds = math.log(prob / (1 - prob)) if prob != 0.5 else 0
                elif direction == 'west':
                    cell.walls['W'].log_odds = math.log(prob / (1 - prob)) if prob != 0.5 else 0
        
        print(f"‚úÖ Occupancy map loaded from JSON ({width}x{height})")
        return occupancy_map
        
    except Exception as e:
        print(f"‚ùå Error loading occupancy map: {e}")
        return None

# =============================================================================
# ===== OBJECT DETECTION FUNCTIONS ==========================================
# =============================================================================

def apply_awb(bgr):
    """Apply automatic white balance"""
    if hasattr(cv2, "xphoto") and hasattr(cv2.xphoto, "createLearningBasedWB"):
        wb = cv2.xphoto.createLearningBasedWB()
        try:
            wb.setSaturationThreshold(0.99)
        except Exception:
            pass
        return wb.balanceWhite(bgr)
    return bgr

def night_enhance_pipeline_cpu(bgr):
    """Night enhancement pipeline"""
    return apply_awb(bgr)

class ObjectTracker:
    def __init__(self, use_gpu=False):
        self.use_gpu = use_gpu
        print(f"üñºÔ∏è  ObjectTracker in {'GPU' if use_gpu else 'CPU'} mode")

    def _get_angle(self, pt1, pt2, pt0):
        dx1 = pt1[0] - pt0[0]; dy1 = pt1[1] - pt0[1]
        dx2 = pt2[0] - pt0[0]; dy2 = pt2[1] - pt0[1]
        dot = dx1*dx2 + dy1*dy2
        mag1 = (dx1*dx1 + dy1*dy1)**0.5
        mag2 = (dx2*dx2 + dy2*dy2)**0.5
        if mag1*mag2 == 0:
            return 0
        return math.degrees(math.acos(max(-1, min(1, dot/(mag1*mag2)))) )

    def get_raw_detections(self, frame):
        enhanced = cv2.GaussianBlur(night_enhance_pipeline_cpu(frame), (5,5), 0)
        hsv = cv2.cvtColor(enhanced, cv2.COLOR_BGR2HSV)

        ranges = {
            'Red': ([0,80,40],[10,255,255],[170,80,40],[180,255,255]),
            'Yellow': ([20,60,40],[35,255,255]),
            'Green': ([35,40,30],[85,255,255]),
            'Blue': ([90,40,30],[130,255,255])
        }
        masks = {}
        masks['Red'] = cv2.inRange(hsv, np.array(ranges['Red'][0]), np.array(ranges['Red'][1])) | \
                       cv2.inRange(hsv, np.array(ranges['Red'][2]), np.array(ranges['Red'][3]))
        for name in ['Yellow','Green','Blue']:
            masks[name] = cv2.inRange(hsv, np.array(ranges[name][0]), np.array(ranges[name][1]))

        combined = masks['Red'] | masks['Yellow'] | masks['Green'] | masks['Blue']
        kernel = np.ones((5,5), np.uint8)
        cleaned = cv2.morphologyEx(cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel), cv2.MORPH_CLOSE, kernel)

        contours,_ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        out = []
        H,W = frame.shape[:2]
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area < 1500: continue
            x,y,w,h = cv2.boundingRect(cnt)
            if w==0 or h==0: continue
            ar = w/float(h)
            if ar>4.0 or ar<0.25: continue
            hull = cv2.convexHull(cnt); ha = cv2.contourArea(hull)
            if ha==0: continue
            solidity = area/ha
            if solidity < 0.85: continue
            if x<=2 or y<=2 or x+w>=W-2 or y+h>=H-2: continue

            contour_mask = np.zeros((H,W), np.uint8)
            cv2.drawContours(contour_mask, [cnt], -1, 255, -1)
            max_mean, found = 0, "Unknown"
            for cname, m in masks.items():
                mv = cv2.mean(m, mask=contour_mask)[0]
                if mv > max_mean:
                    max_mean, found = mv, cname
            if max_mean <= 20: continue

            shape = "Uncertain"
            peri = cv2.arcLength(cnt, True)
            circ = (4*math.pi*area)/(peri*peri) if peri>0 else 0
            if circ > 0.82:
                shape = "Circle"
            else:
                approx = cv2.approxPolyDP(cnt, 0.04*peri, True)
                if len(approx)==4 and solidity>0.88:
                    pts=[tuple(p[0]) for p in approx]
                    angs=[self._get_angle(pts[(i-1)%4], pts[(i+1)%4], p) for i,p in enumerate(pts)]
                    if all(70<=a<=110 for a in angs):
                        _,(rw,rh),_ = cv2.minAreaRect(cnt)
                        if min(rw,rh)>0:
                            ar2 = max(rw,rh)/min(rw,rh)
                            if 0.88<=ar2<=1.12: shape="Square"
                            elif w>h: shape="Rectangle_H"
                            else: shape="Rectangle_V"
            out.append({"contour":cnt,"shape":shape,"color":found,"box":(x,y,w,h)})
        return out

# =============================================================================
# ===== INTEGRATED OBJECT DETECTION SYSTEM ===================================
# =============================================================================

class RMConnection:
    def __init__(self):
        self._lock = threading.Lock()
        self._robot = None
        self.connected = threading.Event()

    def connect(self):
        with self._lock:
            self._safe_close()
            print("ü§ñ Connecting to RoboMaster...")
            rb = robot.Robot()
            rb.initialize(conn_type="ap")
            rb.camera.start_video_stream(display=False, resolution=r_camera.STREAM_540P)
            # subscribe angles
            try:
                rb.gimbal.sub_angle(freq=50, callback=sub_angle_cb)
            except Exception as e:
                print("Gimbal sub_angle error:", e)
            self._robot = rb
            self.connected.set()
            print("‚úÖ RoboMaster connected & camera streaming")

            # recenter gimbal on start
            try:
                rb.gimbal.recenter(pitch_speed=200, yaw_speed=200).wait_for_completed()
            except Exception as e:
                print("Recenter error:", e)

    def _safe_close(self):
        if self._robot is not None:
            try:
                try: self._robot.camera.stop_video_stream()
                except Exception: pass
                try:
                    try: self._robot.gimbal.unsub_angle()
                    except Exception: pass
                except Exception: pass
                try: self._robot.close()
                except Exception: pass
            finally:
                self._robot = None
                self.connected.clear()
                print("üîå Connection closed")

    def drop_and_reconnect(self):
        with self._lock:
            self._safe_close()

    def get_camera(self):
        with self._lock:
            return None if self._robot is None else self._robot.camera

    def get_gimbal(self):
        with self._lock:
            return None if self._robot is None else self._robot.gimbal

    def get_blaster(self):
        with self._lock:
            return None if self._robot is None else self._robot.blaster

    def close(self):
        with self._lock:
            self._safe_close()

def reconnector_thread(manager: RMConnection):
    backoff = 1.0
    while not stop_event.is_set():
        if not manager.connected.is_set():
            try:
                manager.connect()
                backoff = 1.0
            except Exception as e:
                print(f"‚ôªÔ∏è Reconnect failed: {e} (retry in {backoff:.1f}s)")
                time.sleep(backoff)
                backoff = min(backoff*1.6, 8.0)
                continue
        time.sleep(0.2)

def capture_thread_func(manager: RMConnection, q: queue.Queue):
    print("üöÄ Capture thread started")
    fail = 0
    frame_count = 0
    last_success_time = time.time()
    consecutive_errors = 0
    
    while not stop_event.is_set():
        if not manager.connected.is_set():
            time.sleep(0.1)
            continue
            
        cam = manager.get_camera()
        if cam is None:
            time.sleep(0.1)
            continue
            
        try:
            frame = cam.read_cv2_image(timeout=2.0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡∏à‡∏≤‡∏Å 1.0 ‡πÄ‡∏õ‡πá‡∏ô 2.0
            if frame is not None and frame.size > 0:
                # Clear queue if it's full to prevent memory buildup
                if q.full():
                    try: 
                        q.get_nowait()
                    except queue.Empty: 
                        pass
                
                q.put(frame)
                # mark last healthy frame timestamp
                try:
                    global last_frame_received_ts
                    last_frame_received_ts = time.time()
                except Exception:
                    pass
                frame_count += 1
                last_success_time = time.time()
                fail = 0
                consecutive_errors = 0
            else:
                fail += 1
                consecutive_errors += 1
                
        except Exception as e:
            # ‡πÅ‡∏™‡∏î‡∏á error detail ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
            consecutive_errors += 1
            if consecutive_errors <= 3 or consecutive_errors % 10 == 0:
                print(f"‚ö†Ô∏è Camera read error #{consecutive_errors}: {str(e)[:100]}")
            fail += 1

        # More tolerant reconnection policy - ‡∏•‡∏î threshold ‡∏à‡∏≤‡∏Å 20 ‡πÄ‡∏õ‡πá‡∏ô 30
        if fail >= 30:
            print(f"‚ö†Ô∏è Too many camera errors ({fail}) ‚Üí drop & reconnect")
            manager.drop_and_reconnect()
            # Clear queue to prevent memory buildup
            try:
                while True: 
                    q.get_nowait()
            except queue.Empty:
                pass
            fail = 0
            consecutive_errors = 0
            # Longer sleep to allow reconnect
            time.sleep(1.0)
            
        # Tight loop for responsiveness
        time.sleep(0.01)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å 0.005 ‡πÄ‡∏õ‡πá‡∏ô 0.01 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î CPU load
    print("üõë Capture thread stopped")

def processing_thread_func(tracker: ObjectTracker, q: queue.Queue,
                           target_shape, target_color,
                           roi_state,
                           is_detecting_func):
    global processed_output
    print("üß† Processing thread started.")
    processing_count = 0

    while not stop_event.is_set():
        if not is_detecting_func():
            time.sleep(0.2)  # Increased sleep when not detecting
            continue
            
        try:
            frame_to_process = q.get(timeout=0.3)  # Reduced timeout
            processing_count += 1

            # ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô ROI ‡∏ï‡∏≤‡∏° pitch ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            with gimbal_angle_lock:
                pitch_deg = gimbal_angles[0]
            roi_y_dynamic = int(ROI_Y0 - (max(0.0, -pitch_deg) * ROI_SHIFT_PER_DEG))
            roi_y_dynamic = max(ROI_Y_MIN, min(ROI_Y_MAX, roi_y_dynamic))

            ROI_X, ROI_W = roi_state["x"], roi_state["w"]
            ROI_H = roi_state["h"]
            roi_state["y"] = roi_y_dynamic

            roi_frame = frame_to_process[roi_y_dynamic:roi_y_dynamic+ROI_H, ROI_X:ROI_X+ROI_W]
            detections = tracker.get_raw_detections(roi_frame)

            detailed_results = []
            divider1 = int(ROI_W*0.33)
            divider2 = int(ROI_W*0.66)

            object_id_counter = 1
            for d in detections:
                shape, color, (x,y,w,h) = d['shape'], d['color'], d['box']
                endx = x+w
                zone = "Center"
                if endx < divider1: zone = "Left"
                elif x >= divider2: zone = "Right"
                is_target = (shape == target_shape and color == target_color)

                detailed_results.append({
                    "id": object_id_counter,
                    "color": color,
                    "shape": shape,
                    "zone": zone,
                    "is_target": is_target,
                    "box": (x,y,w,h)
                })
                object_id_counter += 1

            with output_lock:
                processed_output = {"details": detailed_results}

        except queue.Empty:
            time.sleep(0.1)  # Sleep when no frames to process
            continue
        except Exception as e:
            print(f"‚ö†Ô∏è Processing error: {e}")
            time.sleep(0.1)  # Increased sleep on error
            # Clear queue to prevent buildup
            try:
                while True: 
                    q.get_nowait()
            except queue.Empty:
                pass

    print("üõë Processing thread stopped.")

def start_detection_mode():
    """Start detection mode for 1 second"""
    global is_detecting_flag, detection_start_time
    is_detecting_flag["v"] = True
    detection_start_time = time.time()
    print("üîç Detection mode activated for 1 second")

def stop_detection_mode():
    """Stop detection mode"""
    global is_detecting_flag
    is_detecting_flag["v"] = False
    print("üîç Detection mode deactivated")

def check_detection_timer():
    """Check if detection mode should be stopped after 1 second"""
    global is_detecting_flag, detection_start_time
    if is_detecting_flag["v"] and detection_start_time is not None:
        if time.time() - detection_start_time >= 1.0:
            stop_detection_mode()
            return True
    return False

def calculate_target_node_from_gimbal(current_pos, robot_direction, gimbal_yaw):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ú‡πà‡∏≤‡∏ô gimbal ‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏´‡∏ô‡∏î‡πÑ‡∏´‡∏ô
    
    Args:
        current_pos: (row, col) ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πà‡∏ô
        robot_direction: 0=North, 1=East, 2=South, 3=West
        gimbal_yaw: -90=Left, 0=Front, +90=Right
    
    Returns:
        (target_row, target_col, target_direction)
    """
    r, c = current_pos
    
    # Direction offset vectors: [North, East, South, West]
    dir_vectors = [(-1, 0), (0, 1), (1, 0), (0, -1)]
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏¥‡∏®‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏±‡∏ô (absolute direction)
    if gimbal_yaw == 0:  # Front
        target_dir = robot_direction
    elif gimbal_yaw == -90:  # Left (counterclockwise)
        target_dir = (robot_direction - 1 + 4) % 4
    elif gimbal_yaw == 90:  # Right (clockwise)
        target_dir = (robot_direction + 1) % 4
    else:
        target_dir = robot_direction  # fallback
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    dr, dc = dir_vectors[target_dir]
    target_pos = (r + dr, c + dc)
    
    return target_pos, target_dir

def adjust_zone_for_absolute_direction(detected_zone, absolute_direction):
    """
    ‡πÅ‡∏õ‡∏•‡∏á zone (Left/Center/Right) ‡∏à‡∏≤‡∏Å camera view ‡πÄ‡∏õ‡πá‡∏ô absolute position
    
    Args:
        detected_zone: "Left", "Center", "Right" ‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
        absolute_direction: 0=North, 1=East, 2=South, 3=West (‡∏ó‡∏¥‡∏®‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏±‡∏ô)
    
    Returns:
        adjusted_zone: zone ‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô absolute position ‡πÅ‡∏•‡πâ‡∏ß
    """
    # Zone system: "Left" = West side, "Right" = East side of cell
    # Center = center of cell (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á)
    
    if detected_zone == "Center":
        return "Center"
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡∏±‡∏ô
    if absolute_direction == 0:  # Facing North
        # Robot's left = West, right = East
        return detected_zone  # No swap needed
        
    elif absolute_direction == 1:  # Facing East  
        # Robot's left = North, right = South
        # ‡πÑ‡∏°‡πà‡∏°‡∏µ North/South zone ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö grid ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô
        return detected_zone
        
    elif absolute_direction == 2:  # Facing South (opposite of North)
        # Robot's left = East, right = West ‚Üí ‡∏ï‡πâ‡∏≠‡∏á swap
        if detected_zone == "Left":
            return "Right"
        elif detected_zone == "Right":
            return "Left"
            
    elif absolute_direction == 3:  # Facing West
        # Robot's left = South, right = North
        # ‡πÑ‡∏°‡πà‡∏°‡∏µ North/South zone ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö grid ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô
        return detected_zone
    
    return detected_zone  # fallback

def scan_all_directions_with_gimbal(scanner, occupancy_map):
    """
    Pan gimbal 3 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á (-90¬∞, 0¬∞, +90¬∞) ‡πÅ‡∏•‡∏∞ detect objects
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏µ‡πà gimbal ‡∏´‡∏±‡∏ô
    ‡∏û‡∏£‡πâ‡∏≠‡∏° distance filtering ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡πá‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏à‡∏≤‡∏Å‡πÇ‡∏´‡∏ô‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    
    Args:
        scanner: EnvironmentScanner object
        occupancy_map: OccupancyGridMap object
    
    Returns:
        total_objects_detected: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
    """
    global processed_output, CURRENT_POSITION, CURRENT_DIRECTION
    
    print("\nüîç === Starting 3-Direction Gimbal Scan ===")
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏∏‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞ scan: ‡∏ã‡πâ‡∏≤‡∏¢, ‡∏´‡∏ô‡πâ‡∏≤, ‡∏Ç‡∏ß‡∏≤
    SCAN_ANGLES = [-90, 0, 90]
    SCAN_NAMES = ["Left", "Front", "Right"]
    STABILIZE_TIME = 0.4  # ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á stabilize
    DETECT_TIME = 0.8     # ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ detect (‡∏•‡∏î‡∏à‡∏≤‡∏Å 1.0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î duplicate)
    
    # Distance filtering thresholds (cm)
    # ‡πÇ‡∏´‡∏ô‡∏î‡πÉ‡∏ô‡∏Å‡∏£‡∏¥‡∏î‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 60cm, ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô:
    # - ‡∏ñ‡πâ‡∏≤ ToF < 50cm ‚Üí ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
    # - ‡∏ñ‡πâ‡∏≤ ToF 50-70cm ‚Üí ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡∏≤‡∏à‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏ö‡πÇ‡∏´‡∏ô‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏ô‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    # - ‡∏ñ‡πâ‡∏≤ ToF > 70cm ‚Üí ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏´‡∏ô‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
    MAX_DETECTION_DISTANCE = 70.0  # ‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    MIN_DETECTION_DISTANCE = 8.0   # ‡∏£‡∏∞‡∏¢‡∏∞‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô sensor error/‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ä‡∏ô‡∏´‡∏∏‡πà‡∏ô)
    
    all_detected = {}  # dict: {node_position: [objects]}
    total_objects = 0
    
    for angle, name in zip(SCAN_ANGLES, SCAN_NAMES):
        print(f"\nüìπ Scanning {name} (Gimbal {angle:+d}¬∞)...")
        
        # ‡∏´‡∏°‡∏∏‡∏ô gimbal
        try:
            scanner.gimbal.moveto(pitch=0, yaw=angle, yaw_speed=SPEED_ROTATE).wait_for_completed(timeout=5.0)
            time.sleep(STABILIZE_TIME)
        except Exception as e:
            print(f"‚ö†Ô∏è Gimbal move error at {angle}¬∞: {e}")
            continue
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏à‡∏≤‡∏Å ToF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö distance filtering
        try:
            if angle == 0:
                # Gimbal ‡∏´‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ ToF ‡∏ï‡∏£‡∏á
                tof_distance = scanner.last_tof_distance_cm
            else:
                # Gimbal ‡∏´‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á ‚Üí ‡∏≠‡πà‡∏≤‡∏ô side ToF (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì
                # ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å ToF ‡∏´‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏°‡∏∑‡πà‡∏≠ gimbal ‡∏´‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á
                # ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏´‡∏£‡∏∑‡∏≠ fallback
                tof_distance = scanner.last_tof_distance_cm
        except Exception:
            tof_distance = 50.0  # fallback ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        
        print(f"   üìè ToF distance: {tof_distance:.1f} cm")
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á healthy ‡πÑ‡∏´‡∏°
        if not camera_is_healthy(timeout=3.0):
            print(f"‚ö†Ô∏è Camera unhealthy at {name} direction, skipping...")
            continue
        
        # ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• detection ‡πÄ‡∏Å‡πà‡∏≤
        with output_lock:
            processed_output["details"] = []
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏° detection
        start_detection_mode()
        time.sleep(DETECT_TIME)
        
        # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        with output_lock:
            objects = processed_output["details"].copy()
        
        stop_detection_mode()
        
        if objects:
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÇ‡∏´‡∏ô‡∏î‡πÑ‡∏´‡∏ô
            target_node, target_direction = calculate_target_node_from_gimbal(
                CURRENT_POSITION, CURRENT_DIRECTION, angle
            )
            
            print(f"   üì¶ Detected {len(objects)} object(s) at {name}")
            print(f"   üìç Target node: {target_node}, Direction: {['N','E','S','W'][target_direction]}")
            
            # Distance filtering: ‡∏Å‡∏£‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏Å‡∏•‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            distance_valid = True
            distance_warning = ""
            
            if angle == 0:  # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≠‡∏ô‡∏´‡∏±‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (ToF ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
                if tof_distance > MAX_DETECTION_DISTANCE:
                    distance_valid = False
                    distance_warning = f" (TOO FAR: {tof_distance:.1f}cm > {MAX_DETECTION_DISTANCE}cm)"
                    print(f"   ‚ö†Ô∏è Distance too far{distance_warning} - Objects likely from next node, SKIPPING")
                elif tof_distance < MIN_DETECTION_DISTANCE:
                    distance_valid = False
                    distance_warning = f" (TOO CLOSE: {tof_distance:.1f}cm < {MIN_DETECTION_DISTANCE}cm)"
                    print(f"   ‚ö†Ô∏è Distance too close{distance_warning} - Possible sensor error, SKIPPING")
                else:
                    print(f"   ‚úÖ Distance valid ({tof_distance:.1f}cm) - Objects in target node")
            else:
                # Gimbal ‡∏´‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á: ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ distance filtering (ToF ‡∏ß‡∏±‡∏î‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á)
                print(f"   ‚ÑπÔ∏è Side scan - Distance filtering disabled")
            
            if not distance_valid:
                # ‡∏£‡∏∞‡∏¢‡∏∞‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô ‚Üí skip objects ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ó‡∏¥‡∏®‡∏ô‡∏µ‡πâ
                continue
            
            # ‡πÅ‡∏õ‡∏•‡∏á zone ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á
            adjusted_objects = []
            for obj in objects:
                adjusted_obj = obj.copy()
                adjusted_obj['zone'] = adjust_zone_for_absolute_direction(
                    obj['zone'], target_direction
                )
                adjusted_objects.append(adjusted_obj)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏
                zone_info = f"in {adjusted_obj['zone']} zone"
                if adjusted_obj['zone'] == 'Left':
                    zone_info += " (West wall)"
                elif adjusted_obj['zone'] == 'Right':
                    zone_info += " (East wall)"
                elif adjusted_obj['zone'] == 'Center':
                    zone_info += " (center)"
                
                target_mark = " ‚≠êTARGET!" if adjusted_obj.get('is_target', False) else ""
                print(f"      ‚Ä¢ {adjusted_obj['color']} {adjusted_obj['shape']} {zone_info}{target_mark}")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏ß‡∏°‡πÑ‡∏ß‡πâ (‡πÄ‡∏ä‡πá‡∏Ñ duplicate ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ scan)
            if target_node not in all_detected:
                all_detected[target_node] = []
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            for adj_obj in adjusted_objects:
                is_duplicate_in_scan = False
                for existing in all_detected[target_node]:
                    if (existing['color'] == adj_obj['color'] and 
                        existing['shape'] == adj_obj['shape'] and 
                        existing['zone'] == adj_obj['zone']):
                        # ‡∏û‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ scan ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ
                        is_duplicate_in_scan = True
                        print(f"      ‚ö†Ô∏è Duplicate in current scan, skipping")
                        break
                
                if not is_duplicate_in_scan:
                    all_detected[target_node].append(adj_obj)
                    total_objects += 1
        else:
            print(f"   üì≠ No objects detected at {name}")
    
    # ‡∏Å‡∏•‡∏±‡∏ö gimbal ‡πÑ‡∏õ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á
    print("\nüîÑ Returning gimbal to center...")
    try:
        scanner.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed(timeout=5.0)
        time.sleep(0.2)
    except Exception as e:
        print(f"‚ö†Ô∏è Gimbal return error: {e}")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏•‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà
    if all_detected:
        print(f"\nüíæ Saving {total_objects} object(s) to map...")
        for node_pos, objects_list in all_detected.items():
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏ô‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡∏≠‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà
            r, c = node_pos
            if 0 <= r < occupancy_map.height and 0 <= c < occupancy_map.width:
                occupancy_map.save_objects_to_map(objects_list, node_pos, CURRENT_DIRECTION)
                print(f"   ‚úÖ Saved {len(objects_list)} object(s) to node {node_pos}")
            else:
                print(f"   ‚ö†Ô∏è Node {node_pos} is out of map bounds, skipping...")
    
    print(f"\nüîç === Gimbal Scan Complete: {total_objects} total objects detected ===\n")
    return total_objects

def save_detected_objects_to_map(occupancy_map):
    """Save detected objects to map with position details in the next cell"""
    global processed_output, CURRENT_POSITION, CURRENT_DIRECTION
    
    with output_lock:
        objects = processed_output["details"]
    
    if objects:
        # Calculate next node position (where robot will move to)
        dir_vectors = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # N, E, S, W
        next_r, next_c = CURRENT_POSITION[0] + dir_vectors[CURRENT_DIRECTION][0], CURRENT_POSITION[1] + dir_vectors[CURRENT_DIRECTION][1]
        
        # Adjust object zones based on robot's facing direction
        # Zone system: "Left" = West side of cell, "Right" = East side of cell
        # Camera detection: "Left"/"Right" relative to robot's view
        adjusted_objects = []
        for obj in objects:
            adjusted_obj = obj.copy()
            
            # Convert robot's relative view to absolute cell position
            # Direction: 0=North(‚Üë), 1=East(‚Üí), 2=South(‚Üì), 3=West(‚Üê)
            
            if CURRENT_DIRECTION == 0:  # Facing North
                # Robot's left = West (cell's "Left"), Robot's right = East (cell's "Right")
                pass  # No swap needed
                
            elif CURRENT_DIRECTION == 1:  # Facing East
                # Robot's left = North, Robot's right = South
                # North/South don't have Left/Right in grid, keep as detected
                pass  # No swap needed
                
            elif CURRENT_DIRECTION == 2:  # Facing South (opposite of North)
                # Robot's left = East (cell's "Right"), Robot's right = West (cell's "Left")
                if obj['zone'] == 'Left':
                    adjusted_obj['zone'] = 'Right'
                elif obj['zone'] == 'Right':
                    adjusted_obj['zone'] = 'Left'
                # Need to swap for South direction
                
            elif CURRENT_DIRECTION == 3:  # Facing West
                # Robot's left = South, Robot's right = North
                # North/South don't have Left/Right in grid, keep as detected
                pass  # No swap needed
            
            adjusted_objects.append(adjusted_obj)
        
        # Save adjusted objects to map at the next position
        occupancy_map.save_objects_to_map(adjusted_objects, (next_r, next_c), CURRENT_DIRECTION)
        
        # Also save to global detected_objects
        with object_lock:
            detected_objects.extend(adjusted_objects)
        
        print(f"‚úÖ Saved {len(adjusted_objects)} objects to map at next position ({next_r}, {next_c})")
        
        # Print detailed object information with zone positioning
        for obj in adjusted_objects:
            zone_info = f"in {obj['zone']} zone"
            if obj['zone'] == 'Left':
                zone_info += " (attached to left wall)"
            elif obj['zone'] == 'Right':
                zone_info += " (attached to right wall)"
            elif obj['zone'] == 'Center':
                zone_info += " (in center of cell)"
            
            print(f"   üì¶ Object: {obj['color']} {obj['shape']} {zone_info} {'(TARGET!)' if obj['is_target'] else ''}")
    else:
        print("üì≠ No objects detected")

# =============================================================================
# ===== OCCUPANCY GRID MAP & VISUALIZATION (from map_suay.py) =================
# =============================================================================

class WallBelief:
    """Class to manage the belief of a 'wall'."""
    def __init__(self):
        self.log_odds = 0.0

    def update(self, is_occupied_reading, sensor_type):
        if is_occupied_reading:
            self.log_odds += LOG_ODDS_OCC[sensor_type]
        else:
            self.log_odds += LOG_ODDS_FREE[sensor_type]
        self.log_odds = max(min(self.log_odds, 10), -10)

    def get_probability(self):
        return 1.0 - 1.0 / (1.0 + math.exp(self.log_odds))

    def is_occupied(self):
        return self.get_probability() > OCCUPANCY_THRESHOLD

class OGMCell:
    """Class for a cell that stores beliefs about 'space' and 'walls'."""
    def __init__(self):
        self.log_odds_occupied = 0.0
        self.walls = {'N': None, 'E': None, 'S': None, 'W': None}

    def get_node_probability(self):
        return 1.0 - 1.0 / (1.0 + math.exp(self.log_odds_occupied))

    def is_node_occupied(self):
        return self.get_node_probability() > OCCUPANCY_THRESHOLD

class OccupancyGridMap:
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.grid = [[OGMCell() for _ in range(width)] for _ in range(height)]
        self._link_walls()

    def _link_walls(self):
        for r in range(self.height):
            for c in range(self.width):
                if self.grid[r][c].walls['N'] is None:
                    wall = WallBelief()
                    self.grid[r][c].walls['N'] = wall
                    if r > 0: self.grid[r-1][c].walls['S'] = wall
                if self.grid[r][c].walls['W'] is None:
                    wall = WallBelief()
                    self.grid[r][c].walls['W'] = wall
                    if c > 0: self.grid[r][c-1].walls['E'] = wall
                if self.grid[r][c].walls['S'] is None:
                    self.grid[r][c].walls['S'] = WallBelief()
                if self.grid[r][c].walls['E'] is None:
                    self.grid[r][c].walls['E'] = WallBelief()

    def update_wall(self, r, c, direction_char, is_occupied_reading, sensor_type):
        if 0 <= r < self.height and 0 <= c < self.width:
            wall = self.grid[r][c].walls.get(direction_char)
            if wall:
                wall.update(is_occupied_reading, sensor_type)

    def update_node(self, r, c, is_occupied_reading, sensor_type='tof'):
        if 0 <= r < self.height and 0 <= c < self.width:
            if is_occupied_reading:
                self.grid[r][c].log_odds_occupied += LOG_ODDS_OCC[sensor_type]
            else:
                self.grid[r][c].log_odds_occupied += LOG_ODDS_FREE[sensor_type]

    def is_path_clear(self, r1, c1, r2, c2):
        dr, dc = r2 - r1, c2 - c1
        if abs(dr) + abs(dc) != 1: return False
        if dr == -1: wall_char = 'N'
        elif dr == 1: wall_char = 'S'
        elif dc == 1: wall_char = 'E'
        elif dc == -1: wall_char = 'W'
        else: return False
        wall = self.grid[r1][c1].walls.get(wall_char)
        if wall and wall.is_occupied(): return False
        if 0 <= r2 < self.height and 0 <= c2 < self.width:
            if self.grid[r2][c2].is_node_occupied(): return False
        else: return False
        return True

    def save_objects_to_map(self, objects, position, direction):
        """Save detected objects to the map at specified position (with duplicate checking)"""
        r, c = position
        if 0 <= r < self.height and 0 <= c < self.width:
            # Add objects to the cell
            if not hasattr(self.grid[r][c], 'objects'):
                self.grid[r][c].objects = []
            
            new_objects_added = 0
            duplicates_skipped = 0
            
            # Add each object with zone information (check for duplicates)
            for obj in objects:
                obj_data = {
                    'color': obj.get('color', 'unknown'),
                    'shape': obj.get('shape', 'unknown'),
                    'zone': obj.get('zone', 'unknown'),
                    'is_target': obj.get('is_target', False),
                    'timestamp': time.time()
                }
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                is_duplicate = False
                for existing_obj in self.grid[r][c].objects:
                    if (existing_obj['color'] == obj_data['color'] and 
                        existing_obj['shape'] == obj_data['shape'] and 
                        existing_obj['zone'] == obj_data['zone']):
                        # ‡∏û‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥ (‡∏™‡∏µ, ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á, zone ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
                        is_duplicate = True
                        duplicates_skipped += 1
                        break
                
                if not is_duplicate:
                    self.grid[r][c].objects.append(obj_data)
                    new_objects_added += 1
            
            if new_objects_added > 0:
                print(f"üì¶ Saved {new_objects_added} new object(s) to cell ({r}, {c})")
            if duplicates_skipped > 0:
                print(f"‚ö†Ô∏è Skipped {duplicates_skipped} duplicate object(s) at cell ({r}, {c})")

class RealTimeVisualizer:
    def __init__(self, grid_size, target_dest=None):
        self.grid_size = grid_size
        self.target_dest = target_dest
        plt.ion()
        self.fig, self.ax = plt.subplots(figsize=MAP_FIGURE_SIZE)
        self.colors = {"robot": "#0000FF", "target": "#FFD700", "path": "#FFFF00", "wall": "#000000", "wall_prob": "#000080"}
        self.obj_color_map = {'Red': '#FF0000', 'Green': '#00FF00', 'Blue': '#0080FF', 'Yellow': '#FFFF00', 'Unknown': '#808080'}

    def update_plot(self, occupancy_map, robot_pos, path=None):
        self.ax.clear()
        self.ax.set_title("Real-time Hybrid Belief Map (Nodes & Walls)")
        self.ax.set_xticks([]); self.ax.set_yticks([])
        self.ax.set_xlim(-0.5, self.grid_size - 0.5)
        self.ax.set_ylim(self.grid_size - 0.5, -0.5)
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                prob = occupancy_map.grid[r][c].get_node_probability()
                if prob > OCCUPANCY_THRESHOLD: color = '#8B0000'
                elif prob < FREE_THRESHOLD: color = '#D3D3D3'
                else: color = '#90EE90'
                self.ax.add_patch(plt.Rectangle((c - 0.5, r - 0.5), 1, 1, facecolor=color, edgecolor='k', lw=0.5))
                self.ax.text(c, r, f"{prob:.2f}", ha="center", va="center", color="black", fontsize=8)
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                cell = occupancy_map.grid[r][c]
                prob_n = cell.walls['N'].get_probability()
                if abs(prob_n - 0.5) > 0.01: self.ax.text(c, r - 0.5, f"{prob_n:.2f}", ha="center", va="center", color=self.colors['wall_prob'], fontsize=6, bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1', edgecolor='none'))
                prob_w = cell.walls['W'].get_probability()
                if abs(prob_w - 0.5) > 0.01: self.ax.text(c - 0.5, r, f"{prob_w:.2f}", ha="center", va="center", color=self.colors['wall_prob'], fontsize=6, rotation=90, bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1', edgecolor='none'))
        for c in range(self.grid_size):
            r_edge = self.grid_size - 1
            prob_s = occupancy_map.grid[r_edge][c].walls['S'].get_probability()
            if abs(prob_s - 0.5) > 0.01: self.ax.text(c, r_edge + 0.5, f"{prob_s:.2f}", ha="center", va="center", color=self.colors['wall_prob'], fontsize=6, bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1', edgecolor='none'))
        for r in range(self.grid_size):
            c_edge = self.grid_size - 1
            prob_e = occupancy_map.grid[r][c_edge].walls['E'].get_probability()
            if abs(prob_e - 0.5) > 0.01: self.ax.text(c_edge + 0.5, r, f"{prob_e:.2f}", ha="center", va="center", color=self.colors['wall_prob'], fontsize=6, rotation=90, bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.1', edgecolor='none'))
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                cell = occupancy_map.grid[r][c]
                if cell.walls['N'].is_occupied(): self.ax.plot([c - 0.5, c + 0.5], [r - 0.5, r - 0.5], color=self.colors['wall'], linewidth=4)
                
                # Display objects in the cell
                if hasattr(cell, 'objects') and cell.objects:
                    for i, obj in enumerate(cell.objects):
                        # Position objects in different zones
                        if obj['zone'] == 'Left':
                            obj_x, obj_y = c - 0.3, r
                        elif obj['zone'] == 'Right':
                            obj_x, obj_y = c + 0.3, r
                        elif obj['zone'] == 'Center':
                            obj_x, obj_y = c, r
                        else:
                            obj_x, obj_y = c, r
                        
                        # Shape-based marker
                        shape = obj.get('shape', 'Uncertain')
                        if shape == 'Circle':
                            marker = 'o'
                        elif shape == 'Square':
                            marker = 's'
                        elif 'Rectangle' in shape:
                            marker = 's'
                        else:
                            marker = 'D'  # Diamond for uncertain
                        
                        # Color based on detected color
                        obj_color = obj.get('color', 'Unknown')
                        color = self.obj_color_map.get(obj_color, '#808080')
                        
                        # Highlight target with red border
                        edge_color = '#FF0000' if obj.get('is_target', False) else 'black'
                        edge_width = 3 if obj.get('is_target', False) else 1
                        
                        self.ax.scatter(obj_x, obj_y, c=color, marker=marker, s=120, edgecolors=edge_color, linewidth=edge_width)
                        
                        # Display ID below the marker
                        obj_id = obj.get('id', '?')
                        self.ax.text(obj_x, obj_y + 0.35, f"ID:{obj_id}", ha="center", va="top", fontsize=7, 
                                    fontweight='bold', bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.2', edgecolor='none'))
                if cell.walls['W'].is_occupied(): self.ax.plot([c - 0.5, c - 0.5], [r - 0.5, r + 0.5], color=self.colors['wall'], linewidth=4)
                if r == self.grid_size - 1 and cell.walls['S'].is_occupied(): self.ax.plot([c - 0.5, c + 0.5], [r + 0.5, r + 0.5], color=self.colors['wall'], linewidth=4)
                if c == self.grid_size - 1 and cell.walls['E'].is_occupied(): self.ax.plot([c + 0.5, c + 0.5], [r - 0.5, r + 0.5], color=self.colors['wall'], linewidth=4)
        if self.target_dest:
            r_t, c_t = self.target_dest
            self.ax.add_patch(plt.Rectangle((c_t - 0.5, r_t - 0.5), 1, 1, facecolor=self.colors['target'], edgecolor='k', lw=2, alpha=0.8))
        if path:
            for r_p, c_p in path: self.ax.add_patch(plt.Rectangle((c_p - 0.5, r_p - 0.5), 1, 1, facecolor=self.colors['path'], edgecolor='k', lw=0.5, alpha=0.7))
        if robot_pos:
            r_r, c_r = robot_pos
            self.ax.add_patch(plt.Rectangle((c_r - 0.5, r_r - 0.5), 1, 1, facecolor=self.colors['robot'], edgecolor='k', lw=2))
        
        # Collect all detected objects with their details
        all_objects = []
        for r in range(self.grid_size):
            for c in range(self.grid_size):
                cell = occupancy_map.grid[r][c]
                if hasattr(cell, 'objects') and cell.objects:
                    for obj in cell.objects:
                        all_objects.append({
                            'id': obj.get('id', '?'),
                            'color': obj.get('color', 'Unknown'),
                            'shape': obj.get('shape', 'Uncertain'),
                            'zone': obj.get('zone', 'Center'),
                            'position': f"({r},{c})",
                            'is_target': obj.get('is_target', False)
                        })
        
        # Display object details on the right side
        if all_objects:
            details_text = "=== DETECTED OBJECTS ===\n\n"
            for obj in sorted(all_objects, key=lambda x: x['id']):
                target_mark = " ‚≠êTARGET" if obj['is_target'] else ""
                details_text += f"ID {obj['id']}: {obj['color']} {obj['shape']}\n"
                details_text += f"  @ {obj['position']} [{obj['zone']}]{target_mark}\n\n"
            
            # Add text box on the right side
            self.fig.text(0.78, 0.5, details_text, fontsize=9, family='monospace',
                         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))
        
        legend_elements = [ 
            plt.Rectangle((0,0),1,1, facecolor='#8B0000', label=f'Node Occupied (P>{OCCUPANCY_THRESHOLD})'), 
            plt.Rectangle((0,0),1,1, facecolor='#90EE90', label=f'Node Unknown'), 
            plt.Rectangle((0,0),1,1, facecolor='#D3D3D3', label=f'Node Free (P<{FREE_THRESHOLD})'), 
            plt.Line2D([0], [0], color=self.colors['wall'], lw=4, label='Wall Occupied'), 
            plt.Rectangle((0,0),1,1, facecolor=self.colors['robot'], label='Robot'), 
            plt.Rectangle((0,0),1,1, facecolor=self.colors['target'], label='Target') 
        ]
        self.ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.0, 1.0), fontsize=8)
        self.fig.tight_layout(rect=[0, 0, 0.75, 1])
        self.fig.canvas.draw(); self.fig.canvas.flush_events(); plt.pause(0.01)

# =============================================================================
# ===== CORE ROBOT CONTROL CLASSES ============================================
# =============================================================================
class AttitudeHandler:
    def __init__(self):
        self.current_yaw, self.yaw_tolerance, self.is_monitoring = 0.0, 3.0, False
    def attitude_handler(self, attitude_info):
        if self.is_monitoring: self.current_yaw = attitude_info[0]
    def start_monitoring(self, chassis):
        self.is_monitoring = True; chassis.sub_attitude(freq=20, callback=self.attitude_handler)
    def stop_monitoring(self, chassis):
        self.is_monitoring = False;
        try: chassis.unsub_attitude()
        except Exception: pass
    def normalize_angle(self, angle):
        while angle > 180: angle -= 360
        while angle <= -180: angle += 360
        return angle
    def correct_yaw_to_target(self, chassis, target_yaw=0.0):
        normalized_target = self.normalize_angle(target_yaw); time.sleep(0.1)
        robot_rotation = -self.normalize_angle(normalized_target - self.current_yaw)
        print(f"\nüîß Correcting Yaw: {self.current_yaw:.1f}¬∞ -> {target_yaw:.1f}¬∞. Rotating: {robot_rotation:.1f}¬∞")
        if abs(robot_rotation) > self.yaw_tolerance:
            chassis.move(x=0, y=0, z=robot_rotation, z_speed=60).wait_for_completed(timeout=2)
            time.sleep(0.1)
        final_error = abs(self.normalize_angle(normalized_target - self.current_yaw))
        if final_error <= self.yaw_tolerance: print(f"‚úÖ Yaw Correction Success: {self.current_yaw:.1f}¬∞"); return True
        print(f"‚ö†Ô∏è First attempt incomplete. Current: {self.current_yaw:.1f}¬∞. Fine-tuning...")
        remaining_rotation = -self.normalize_angle(normalized_target - self.current_yaw)
        if abs(remaining_rotation) > 0.5 and abs(remaining_rotation) < 20:
            chassis.move(x=0, y=0, z=remaining_rotation, z_speed=40).wait_for_completed(timeout=2)
            time.sleep(0.1)
        final_error = abs(self.normalize_angle(normalized_target - self.current_yaw))
        if final_error <= self.yaw_tolerance: print(f"‚úÖ Yaw Fine-tuning Success: {self.current_yaw:.1f}¬∞"); return True
        else: print(f"üî•üî• Yaw Correction FAILED. Final Yaw: {self.current_yaw:.1f}¬∞"); return False

class PID:
    def __init__(self, Kp, Ki, Kd, setpoint=0):
        self.Kp, self.Ki, self.Kd, self.setpoint = Kp, Ki, Kd, setpoint
        self.prev_error, self.integral, self.integral_max = 0, 0, 1.0
    def compute(self, current, dt):
        error = self.setpoint - current
        self.integral += error * dt; self.integral = max(min(self.integral, self.integral_max), -self.integral_max)
        derivative = (error - self.prev_error) / dt if dt > 0 else 0
        output = self.Kp * error + self.Ki * self.integral + self.Kd * derivative
        self.prev_error = error; return output

class MovementController:
    def __init__(self, chassis):
        self.chassis = chassis
        self.current_x_pos, self.current_y_pos = 0.0, 0.0
        self.chassis.sub_position(freq=20, callback=self.position_handler)
    def position_handler(self, position_info):
        self.current_x_pos, self.current_y_pos = position_info[0], position_info[1]

    def _calculate_yaw_correction(self, attitude_handler, target_yaw):
        KP_YAW = 0.8; MAX_YAW_SPEED = 25
        yaw_error = attitude_handler.normalize_angle(target_yaw - attitude_handler.current_yaw)
        speed = KP_YAW * yaw_error
        return max(min(speed, MAX_YAW_SPEED), -MAX_YAW_SPEED)

    def move_forward_one_grid(self, axis, attitude_handler):
        attitude_handler.correct_yaw_to_target(self.chassis, get_compensated_target_yaw()) # MODIFIED
        target_distance = 0.6
        pid = PID(Kp=0.5, Ki=0.1, Kd=32, setpoint=target_distance)
        start_time, last_time = time.time(), time.time()
        start_position = self.current_x_pos if axis == 'x' else self.current_y_pos
        print(f"üöÄ Moving FORWARD 0.6m, monitoring GLOBAL AXIS '{axis}'")
        while time.time() - start_time < 3.5: # Increased timeout
            now = time.time(); dt = now - last_time; last_time = now
            current_position = self.current_x_pos if axis == 'x' else self.current_y_pos
            relative_position = abs(current_position - start_position)
            if abs(relative_position - target_distance) < 0.03:
                print("\n‚úÖ Move complete!"); break
            output = pid.compute(relative_position, dt)
            ramp_multiplier = min(1.0, 0.1 + ((now - start_time) / 1.0) * 0.9)
            speed = max(-0.7, min(0.7, output * ramp_multiplier))
            yaw_correction = self._calculate_yaw_correction(attitude_handler, get_compensated_target_yaw()) # MODIFIED
            self.chassis.drive_speed(x=speed, y=0, z=yaw_correction, timeout=1)
            print(f"Moving... Dist: {relative_position:.3f}/{target_distance:.2f} m", end='\r')
        self.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0); time.sleep(0.5)

    def adjust_position_to_wall(self, sensor_adaptor, attitude_handler, side, sensor_config, target_distance_cm, direction_multiplier):
        compensated_yaw = get_compensated_target_yaw() # MODIFIED
        print(f"\n--- Adjusting {side} Side (Yaw locked at {compensated_yaw:.2f}¬∞) ---") # MODIFIED
        print(f"   -> Config: ID={sensor_config['sharp_id']}, Port={sensor_config['sharp_port']}, Target={target_distance_cm}cm")
        TOLERANCE_CM, MAX_EXEC_TIME, KP_SLIDE, MAX_SLIDE_SPEED = 0.8, 8, 0.045, 0.18
        start_time = time.time()
        while time.time() - start_time < MAX_EXEC_TIME:
            adc_val = sensor_adaptor.get_adc(id=sensor_config["sharp_id"], port=sensor_config["sharp_port"])
            current_dist = convert_adc_to_cm(adc_val)
            dist_error = target_distance_cm - current_dist
            if abs(dist_error) <= TOLERANCE_CM:
                print(f"\n[{side}] Target distance reached! Final distance: {current_dist:.2f} cm")
                break
            slide_speed = max(min(direction_multiplier * KP_SLIDE * dist_error, MAX_SLIDE_SPEED), -MAX_SLIDE_SPEED)
            yaw_correction = self._calculate_yaw_correction(attitude_handler, compensated_yaw) # MODIFIED
            self.chassis.drive_speed(x=0, y=slide_speed, z=yaw_correction)
            print(f"Adjusting {side}... Current: {current_dist:5.2f}cm, Target: {target_distance_cm:4.1f}cm, Error: {dist_error:5.2f}cm, Speed: {slide_speed:5.3f}", end='\r')
            time.sleep(0.02)
        else:
            print(f"\n[{side}] Movement timed out!")
        self.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
        time.sleep(0.1)

    def center_in_node_with_tof(self, scanner, attitude_handler, target_cm=19, tol_cm=1.0, max_adjust_time=6.0):
        """
        REVISED: Now respects the global activity lock from the scanner.
        It will not run if a side-scan operation is in progress.
        """
        # [CRITICAL] Guard Clause to respect the global lock
        if scanner.is_performing_full_scan:
            print("[ToF Centering] SKIPPED: A critical side-scan is in progress.")
            return

        print("\n--- Stage: Centering in Node with ToF ---")
        time.sleep(0.1)
        tof_dist = scanner.last_tof_distance_cm
        if tof_dist is None or math.isinf(tof_dist):
            print("[ToF] ‚ùå No valid ToF data available. Skipping centering.")
            return
        print(f"[ToF] Initial front distance: {tof_dist:.2f} cm")
        if tof_dist >= 50:
            print("[ToF] Distance >= 50cm, likely in an open space. Skipping centering.")
            return
        direction = 0
        if tof_dist > target_cm + tol_cm:
            print("[ToF] Too far from front wall. Moving forward...")
            direction = abs(TOF_ADJUST_SPEED)
        elif tof_dist < 22:
            print("[ToF] Too close to front wall. Moving backward...")
            direction = -abs(TOF_ADJUST_SPEED)
        else:
            print("[ToF] In range (22cm - target), but not centered. Moving forward...")
            direction = abs(TOF_ADJUST_SPEED)
        if direction == 0:
            print(f"[ToF] Already centered within tolerance ({tof_dist:.2f} cm). Skipping.")
            return
        start_time = time.time()
        compensated_yaw = get_compensated_target_yaw() # MODIFIED
        while time.time() - start_time < max_adjust_time:
            yaw_correction = self._calculate_yaw_correction(attitude_handler, compensated_yaw) # MODIFIED
            self.chassis.drive_speed(x=direction, y=0, z=yaw_correction, timeout=0.1)
            time.sleep(0.12)
            self.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
            time.sleep(0.08)
            current_tof = scanner.last_tof_distance_cm
            if current_tof is None or math.isinf(current_tof):
                continue
            print(f"[ToF] Adjusting... Current Distance: {current_tof:.2f} cm", end="\r")
            if abs(current_tof - target_cm) <= tol_cm:
                print(f"\n[ToF] ‚úÖ Centering complete. Final distance: {current_tof:.2f} cm")
                break
            if (direction > 0 and current_tof < target_cm - tol_cm) or \
            (direction < 0 and current_tof > target_cm + tol_cm):
                direction *= -1
                print("\n[ToF] üîÑ Overshot target. Reversing direction for fine-tuning.")
        else:
            print(f"\n[ToF] ‚ö†Ô∏è Centering timed out. Final distance: {scanner.last_tof_distance_cm:.2f} cm")
        self.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
        time.sleep(0.1)

    def rotate_to_direction(self, target_direction, attitude_handler):
        global CURRENT_DIRECTION
        if CURRENT_DIRECTION == target_direction: return
        diff = (target_direction - CURRENT_DIRECTION + 4) % 4
        if diff == 1: self.rotate_90_degrees_right(attitude_handler)
        elif diff == 3: self.rotate_90_degrees_left(attitude_handler)
        elif diff == 2: self.rotate_90_degrees_right(attitude_handler); self.rotate_90_degrees_right(attitude_handler)

    def rotate_90_degrees_right(self, attitude_handler):
        global CURRENT_TARGET_YAW, CURRENT_DIRECTION, ROBOT_FACE
        print("üîÑ Rotating 90¬∞ RIGHT...")
        CURRENT_TARGET_YAW = attitude_handler.normalize_angle(CURRENT_TARGET_YAW + 90)
        attitude_handler.correct_yaw_to_target(self.chassis, get_compensated_target_yaw()) # MODIFIED
        CURRENT_DIRECTION = (CURRENT_DIRECTION + 1) % 4; ROBOT_FACE += 1
    def rotate_90_degrees_left(self, attitude_handler):
        global CURRENT_TARGET_YAW, CURRENT_DIRECTION, ROBOT_FACE
        print("üîÑ Rotating 90¬∞ LEFT...")
        CURRENT_TARGET_YAW = attitude_handler.normalize_angle(CURRENT_TARGET_YAW - 90)
        attitude_handler.correct_yaw_to_target(self.chassis, get_compensated_target_yaw()) # MODIFIED
        CURRENT_DIRECTION = (CURRENT_DIRECTION - 1 + 4) % 4; ROBOT_FACE -= 1
        if ROBOT_FACE < 1: ROBOT_FACE += 4
    def cleanup(self):
        try: self.chassis.unsub_position()
        except Exception: pass

class EnvironmentScanner:
    """ 
    REVISED: Added a global activity lock 'is_performing_full_scan' to prevent
    any other function from interfering during the complex side-scanning process.
    """
    def __init__(self, sensor_adaptor, tof_sensor, gimbal, chassis):
        self.sensor_adaptor, self.tof_sensor, self.gimbal, self.chassis = sensor_adaptor, tof_sensor, gimbal, chassis
        self.tof_wall_threshold_cm = 60.0
        
        # --- State Management Variables ---
        self.last_tof_distance_cm = float('inf')  # Stores the FRONT distance
        self.side_tof_reading_cm = float('inf')   # [NEW] Temporary storage for side readings
        self.is_gimbal_centered = True            # [NEW] State flag to control the callback
        
        # --- [NEW] Global Activity Lock ---
        self.is_performing_full_scan = False

        # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô ToF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î error ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô timeout
        try:
            self.tof_sensor.sub_distance(freq=5, callback=self._tof_data_handler)
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: ToF subscription error: {e}")
            print("   Continuing without ToF subscription...")
        
        self.side_sensors = {
            "Left":  { "sharp_id": 1, "sharp_port": 1, "ir_id": 1, "ir_port": 2 },
            "Right": { "sharp_id": 2, "sharp_port": 1, "ir_id": 2, "ir_port": 2 }
        }

    def _tof_data_handler(self, sub_info):
        """ 
        MODIFIED: This callback now respects the 'is_gimbal_centered' flag.
        It only updates the main front distance variable if the gimbal is facing forward.
        Otherwise, it updates a separate variable for side readings.
        """
        try:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
            if sub_info and len(sub_info) > 0 and sub_info[0] is not None:
                calibrated_cm = calibrate_tof_value(sub_info[0])
                if self.is_gimbal_centered:
                    self.last_tof_distance_cm = calibrated_cm
                else:
                    self.side_tof_reading_cm = calibrated_cm
        except Exception as e:
            # ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö error ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            pass

    def _get_stable_reading_cm(self, side, duration=0.35):
        sensor_info = self.side_sensors.get(side)
        if not sensor_info: return None, None
        readings = []
        start_time = time.time()
        while time.time() - start_time < duration:
            adc = self.sensor_adaptor.get_adc(id=sensor_info["sharp_id"], port=sensor_info["sharp_port"])
            readings.append(convert_adc_to_cm(adc))
            time.sleep(0.05)
        if len(readings) < 5: return None, None
        return statistics.mean(readings), statistics.stdev(readings)

    def get_sensor_readings(self):
        """
        REVISED: Now uses a global lock 'is_performing_full_scan' to ensure
        this entire operation is atomic and uninterruptible.
        """
        # [CRITICAL] Set the global lock at the very beginning
        self.is_performing_full_scan = True
        try:
            # Center gimbal with error handling
            try:
                self.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed(timeout=5.0)
                time.sleep(0.2)
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Gimbal center error: {e}")
                time.sleep(0.3)
            
            readings = {}
            readings['front'] = (self.last_tof_distance_cm < self.tof_wall_threshold_cm)
            print(f"[SCAN] Front (ToF): {self.last_tof_distance_cm:.1f}cm -> {'OCCUPIED' if readings['front'] else 'FREE'}")
            
            for side in ["Left", "Right"]:
                avg_dist, std_dev = self._get_stable_reading_cm(side)
                if avg_dist is None:
                    print(f"[{side.upper()}] Wall Check Error: Not enough sensor data.")
                    readings[side.lower()] = False
                    continue
                
                is_sharp_detecting_wall = (avg_dist < SHARP_WALL_THRESHOLD_CM and std_dev < SHARP_STDEV_THRESHOLD)
                ir_value = self.sensor_adaptor.get_io(id=self.side_sensors[side]["ir_id"], port=self.side_sensors[side]["ir_port"])
                is_ir_detecting_wall = (ir_value == 0)

                print(f"\n[SCAN] {side} Side Analysis:")
                print(f"    -> Sharp -> Suggests: {'WALL' if is_sharp_detecting_wall else 'FREE'}")
                print(f"    -> IR    -> Suggests: {'WALL' if is_ir_detecting_wall else 'FREE'}")

                if is_sharp_detecting_wall == is_ir_detecting_wall:
                    is_wall = is_sharp_detecting_wall
                    print(f"    -> Decision: Sensors agree. Result is {'WALL' if is_wall else 'FREE'}.")
                else:
                    print("    -> Ambiguity detected! Confirming with ToF...")
                    target_gimbal_yaw = -90 if side == "Left" else 90
                    
                    try:
                        self.is_gimbal_centered = False
                        t_start = time.time()
                        try:
                            self.gimbal.moveto(pitch=0, yaw=target_gimbal_yaw, yaw_speed=SPEED_ROTATE).wait_for_completed(timeout=5.0)
                        except Exception as gimbal_err:
                            print(f"    ‚ö†Ô∏è Gimbal move error: {gimbal_err}")
                        t_gimbal = time.time() - t_start
                        if t_gimbal > 3.0:
                            print(f"    ‚ö†Ô∏è Gimbal move took {t_gimbal:.2f}s (unusually long!)")
                        time.sleep(0.4)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 0.4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤ stabilize ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
                        
                        # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤ ToF ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏•‡∏î‡∏à‡∏≤‡∏Å 3 ‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
                        tof_readings = []
                        for i in range(2):  # ‡∏≠‡πà‡∏≤‡∏ô 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                            tof_readings.append(self.side_tof_reading_cm)
                            if i < 1:  # ‡πÑ‡∏°‡πà sleep ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                                time.sleep(0.1)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô 0.1
                        
                        # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô
                        tof_confirm_dist_cm = sum(tof_readings) / len(tof_readings)
                        print(f"    -> ToF readings at {target_gimbal_yaw}¬∞: {[f'{r:.1f}' for r in tof_readings]} cm")
                        print(f"    -> Average ToF reading: {tof_confirm_dist_cm:.2f} cm.")
                        
                        is_wall = (tof_confirm_dist_cm < self.tof_wall_threshold_cm)
                        print(f"    -> ToF Confirmation: {'WALL DETECTED' if is_wall else 'NO WALL'}.")
                    
                    finally:
                        try:
                            self.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed(timeout=5.0)
                        except Exception as gimbal_err:
                            print(f"    ‚ö†Ô∏è Gimbal return to center error: {gimbal_err}")
                        self.is_gimbal_centered = True
                        time.sleep(0.2)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á stabilize

                readings[side.lower()] = is_wall
                print(f"    -> Final Result for {side} side: {'WALL' if is_wall else 'FREE'}")
            
            return readings
        finally:
            # [CRITICAL] Release the global lock when the function is completely done
            self.is_performing_full_scan = False

    def get_front_tof_cm(self):
        self.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed()
        time.sleep(0.1)
        return self.last_tof_distance_cm

    def cleanup(self):
        try: self.tof_sensor.unsub_distance()
        except Exception: pass

# =============================================================================
# ===== PATHFINDING & EXPLORATION LOGIC =======================================
# =============================================================================

def find_path_bfs(occupancy_map, start, end):
    queue = deque([[start]]); visited = {start}
    moves = [(-1, 0), (0, 1), (1, 0), (0, -1)] # N, E, S, W
    while queue:
        path = queue.popleft()
        r, c = path[-1]
        if (r, c) == end: return path
        for dr, dc in moves:
            nr, nc = r + dr, c + dc
            if 0 <= nr < occupancy_map.height and 0 <= nc < occupancy_map.width:
                if occupancy_map.is_path_clear(r, c, nr, nc) and (nr, nc) not in visited:
                    visited.add((nr, nc))
                    new_path = list(path)
                    new_path.append((nr, nc))
                    queue.append(new_path)
    return None

def find_nearest_unvisited_path(occupancy_map, start_pos, visited_cells):
    h, w = occupancy_map.height, occupancy_map.width
    unvisited_cells_coords = []
    for r in range(h):
        for c in range(w):
            if (r, c) not in visited_cells and not occupancy_map.grid[r][c].is_node_occupied():
                unvisited_cells_coords.append((r, c))
    if not unvisited_cells_coords: return None
    shortest_path = None
    for target_pos in unvisited_cells_coords:
        path = find_path_bfs(occupancy_map, start_pos, target_pos)
        if path:
            if shortest_path is None or len(path) < len(shortest_path):
                shortest_path = path
    return shortest_path

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô execute_path

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô execute_path

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô execute_path

def execute_path(path, movement_controller, attitude_handler, scanner, visualizer, occupancy_map, path_name="Backtrack"):
    global CURRENT_POSITION
    print(f"üéØ Executing {path_name} Path: {path}")
    dir_vectors_map = {(-1, 0): 0, (0, 1): 1, (1, 0): 2, (0, -1): 3}
    dir_map_abs_char = {0: 'N', 1: 'E', 2: 'S', 3: 'W'}
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á path execution
    log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, f"{path_name}_start")

    for i in range(len(path) - 1):
        visualizer.update_plot(occupancy_map, path[i], path)
        current_r, current_c = path[i]
        
        if i + 1 < len(path):
            next_r, next_c = path[i+1]
            dr, dc = next_r - current_r, next_c - current_c
            
            target_direction = dir_vectors_map[(dr, dc)]
            
            movement_controller.rotate_to_direction(target_direction, attitude_handler)
            
            # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ---
            print(f"   -> [{path_name}] Confirming path to ({next_r},{next_c}) with ToF...")
            scanner.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed()
            time.sleep(0.15)
            
            # 1. ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏à‡∏£‡∏¥‡∏á
            is_blocked = scanner.get_front_tof_cm() < scanner.tof_wall_threshold_cm
            
            # 2. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏á
            occupancy_map.update_wall(current_r, current_c, dir_map_abs_char[CURRENT_DIRECTION], is_blocked, 'tof')
            print(f"   -> [{path_name}] Real-time ToF check: Path is {'BLOCKED' if is_blocked else 'CLEAR'}.")
            visualizer.update_plot(occupancy_map, CURRENT_POSITION)

            # 3. <<<<<<<<<<<<<<<<<<<< ‡∏à‡∏∏‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç >>>>>>>>>>>>>>>>>>>>
            #    ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà ‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á!
            if is_blocked:
                print(f"   -> üî• [{path_name}] IMMEDIATE STOP. Real-time sensor detected an obstacle. Aborting path.")
                break # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

            axis_to_monitor = 'x' if ROBOT_FACE % 2 != 0 else 'y'
            movement_controller.move_forward_one_grid(axis=axis_to_monitor, attitude_handler=attitude_handler)
            
            movement_controller.center_in_node_with_tof(scanner, attitude_handler)

            CURRENT_POSITION = (next_r, next_c)
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô path execution
            log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, f"{path_name}_moved")
            visualizer.update_plot(occupancy_map, CURRENT_POSITION, path)
            
            print(f"   -> [{path_name}] Performing side alignment at new position {CURRENT_POSITION}")
            perform_side_alignment_and_mapping(movement_controller, scanner, attitude_handler, occupancy_map, visualizer)
            
            # --- OBJECT DETECTION AFTER BACKTRACK MOVEMENT ---
            # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏´‡∏•‡∏±‡∏á backtrack ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô
            if not camera_is_healthy():
                print(f"üõë [{path_name}] Camera unhealthy ‚Üí locking chassis and waiting...")
                try:
                    movement_controller.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
                except Exception:
                    pass
                wait_start = time.time()
                while not camera_is_healthy():
                    if time.time() - wait_start > 30.0:
                        print(f"‚ö†Ô∏è [{path_name}] Camera recovery timeout (30s). Forcing reconnect and continuing wait...")
                        manager.drop_and_reconnect()
                        wait_start = time.time()
                    time.sleep(0.2)
                print(f"‚úÖ [{path_name}] Camera recovered. Continuing...")
            
            # ‡πÉ‡∏ä‡πâ Gimbal Scan ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á 3 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
            print(f"üîç [{path_name}] Performing 3-direction gimbal scan at new position...")
            if camera_is_healthy():
                scan_all_directions_with_gimbal(scanner, occupancy_map)
                print(f"üîç [{path_name}] Gimbal scan completed at new position")
            else:
                print(f"üìπ [{path_name}] Camera not ready - Skipping gimbal scan")

    print(f"‚úÖ {path_name} complete.")

def perform_side_alignment_and_mapping(movement_controller, scanner, attitude_handler, occupancy_map, visualizer):
    print("\n--- Stage: Wall Detection & Side Alignment ---")
    r, c = CURRENT_POSITION
    dir_map_abs_char = {0: 'N', 1: 'E', 2: 'S', 3: 'W'}
    
    side_walls_present = scanner.get_sensor_readings()

    left_dir_abs = (CURRENT_DIRECTION - 1 + 4) % 4
    occupancy_map.update_wall(r, c, dir_map_abs_char[left_dir_abs], side_walls_present['left'], 'sharp')
    visualizer.update_plot(occupancy_map, CURRENT_POSITION)
    if side_walls_present['left']:
        movement_controller.adjust_position_to_wall(
            scanner.sensor_adaptor, attitude_handler, "Left", 
            scanner.side_sensors["Left"], LEFT_TARGET_CM, direction_multiplier=1
        )
    
    right_dir_abs = (CURRENT_DIRECTION + 1) % 4
    occupancy_map.update_wall(r, c, dir_map_abs_char[right_dir_abs], side_walls_present['right'], 'sharp')
    visualizer.update_plot(occupancy_map, CURRENT_POSITION)
    if side_walls_present['right']:
        movement_controller.adjust_position_to_wall(
            scanner.sensor_adaptor, attitude_handler, "Right", 
            scanner.side_sensors["Right"], RIGHT_TARGET_CM, direction_multiplier=-1
        )

    if not side_walls_present['left'] and not side_walls_present['right']:
        print("\n‚ö†Ô∏è  WARNING: No side walls detected. Skipping alignment.")
    
    attitude_handler.correct_yaw_to_target(movement_controller.chassis, get_compensated_target_yaw()) # MODIFIED
    time.sleep(0.05)


def explore_with_ogm(scanner, movement_controller, attitude_handler, occupancy_map, visualizer, max_steps=40):
    global CURRENT_POSITION, CURRENT_DIRECTION, IMU_DRIFT_COMPENSATION_DEG
    visited_cells = set()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, "exploration_start")
    
    for step in range(max_steps):
        if not camera_is_healthy():
            if not wait_for_camera_recovery(pause_label=f"Step {step+1}", max_wait=90.0):
                print(f"‚ö†Ô∏è Step {step+1}: Camera recovery failed, continuing anyway...")

        r, c = CURRENT_POSITION
        print(f"\n--- Step {step + 1} at {CURRENT_POSITION}, Facing: {['N', 'E', 'S', 'W'][CURRENT_DIRECTION]} ---")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ step
        log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, f"step_{step + 1}")
        
        print("   -> Resetting Yaw to ensure perfect alignment before new step...")
        attitude_handler.correct_yaw_to_target(movement_controller.chassis, get_compensated_target_yaw()) # MODIFIED
        
        perform_side_alignment_and_mapping(movement_controller, scanner, attitude_handler, occupancy_map, visualizer)

        print("--- Performing Scan for Mapping (Front ToF Only) ---")
        is_front_occupied = scanner.get_front_tof_cm() < scanner.tof_wall_threshold_cm
        dir_map_abs_char = {0: 'N', 1: 'E', 2: 'S', 3: 'W'}
        occupancy_map.update_wall(r, c, dir_map_abs_char[CURRENT_DIRECTION], is_front_occupied, 'tof')
        
        occupancy_map.update_node(r, c, False, 'tof')
        visited_cells.add((r, c))
        visualizer.update_plot(occupancy_map, CURRENT_POSITION)
        
        # --- NEW: Update IMU Drift Compensation ---
        nodes_visited = len(visited_cells)
        if nodes_visited >= IMU_COMPENSATION_START_NODE_COUNT:
            # Calculate how many intervals have passed
            compensation_intervals = nodes_visited // IMU_COMPENSATION_NODE_INTERVAL
            new_compensation = compensation_intervals * IMU_COMPENSATION_DEG_PER_INTERVAL
            if new_compensation != IMU_DRIFT_COMPENSATION_DEG:
                IMU_DRIFT_COMPENSATION_DEG = new_compensation
                print(f"üî© IMU Drift Compensation Updated: Visited {nodes_visited} nodes. New offset is {IMU_DRIFT_COMPENSATION_DEG:.1f}¬∞")
        # --- END OF NEW CODE ---
        
        priority_dirs = [(CURRENT_DIRECTION + 1) % 4, CURRENT_DIRECTION, (CURRENT_DIRECTION - 1 + 4) % 4]
        moved = False
        dir_vectors = [(-1, 0), (0, 1), (1, 0), (0, -1)]
        
        for target_dir in priority_dirs:
            target_r, target_c = r + dir_vectors[target_dir][0], c + dir_vectors[target_dir][1]
            
            if occupancy_map.is_path_clear(r, c, target_r, target_c) and (target_r, target_c) not in visited_cells:
                print(f"Path to {['N','E','S','W'][target_dir]} at ({target_r},{target_c}) seems clear. Attempting move.")
                movement_controller.rotate_to_direction(target_dir, attitude_handler)
                
                # <<< NEW CODE ADDED >>>
                # Ensure the gimbal is facing forward before checking the path and moving.
                print("    Ensuring gimbal is centered before ToF confirmation...")
                scanner.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed();
                time.sleep(0.1)
                # <<< END OF NEW CODE >>>
                
                # Check camera health before ToF confirmation
                if not camera_is_healthy(timeout=3.0):
                    print("üõë Camera unhealthy before path confirmation ‚Üí waiting for recovery...")
                    if not wait_for_camera_recovery(pause_label="Path Confirmation (explore_with_ogm)", max_wait=60.0):
                        print("‚ö†Ô∏è Camera recovery failed, but continuing...")
                
                print("    Confirming path forward with ToF...")
                is_blocked = scanner.get_front_tof_cm() < scanner.tof_wall_threshold_cm
                
                occupancy_map.update_wall(r, c, dir_map_abs_char[CURRENT_DIRECTION], is_blocked, 'tof')
                print(f"    ToF confirmation: Wall belief updated. Path is {'BLOCKED' if is_blocked else 'CLEAR'}.")
                visualizer.update_plot(occupancy_map, CURRENT_POSITION)
                
                # <<< NEW: Double-check with ToF after rotation >>>
                if is_blocked:
                    print(f"    üö´ Wall detected! Turning back to original direction and recalculating path...")
                    movement_controller.rotate_to_direction(CURRENT_DIRECTION, attitude_handler)
                    print(f"    ‚úÖ Turned back to {['N','E','S','W'][CURRENT_DIRECTION]}. Re-evaluating available paths...")
                    continue  # Skip this direction and try next one
                # <<< END OF NEW CODE >>>
                
                if occupancy_map.is_path_clear(r, c, target_r, target_c):
                    # Check camera health before moving
                    if not camera_is_healthy(timeout=3.0):
                        print("üõë Camera unhealthy before movement ‚Üí waiting for recovery...")
                        if not wait_for_camera_recovery(pause_label="Before Movement", max_wait=60.0):
                            print("‚ö†Ô∏è Camera recovery failed, but continuing with movement...")
                    
                    axis_to_monitor = 'x' if ROBOT_FACE % 2 != 0 else 'y'
                    movement_controller.move_forward_one_grid(axis=axis_to_monitor, attitude_handler=attitude_handler)
                
                    movement_controller.center_in_node_with_tof(scanner, attitude_handler)
                
                    CURRENT_POSITION = (target_r, target_c)
                    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà
                    log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, "moved_to_new_node")
                    moved = True
                    break
                else:
                    print(f"    Confirmation failed. Path to {['N','E','S','W'][target_dir]} is blocked. Re-evaluating.")
        
        if not moved:
            print("No immediate unvisited path. Initiating backtrack...")
            backtrack_path = find_nearest_unvisited_path(occupancy_map, CURRENT_POSITION, visited_cells)
            
            if backtrack_path and len(backtrack_path) > 1:
                execute_path(backtrack_path, movement_controller, attitude_handler, scanner, visualizer, occupancy_map)
                print("Backtrack to new area complete. Resuming exploration.")
                continue
            else:
                print("üéâ EXPLORATION COMPLETE! No reachable unvisited cells remain.")
                break
        # end of per-step block
    
    print("\nüéâ === EXPLORATION PHASE FINISHED ===")

# =============================================================================
# ===== MAIN EXECUTION BLOCK ==================================================
# =============================================================================
if __name__ == '__main__':
    ep_robot = None
    occupancy_map = None
    attitude_handler = AttitudeHandler()
    movement_controller = None
    scanner = None
    ep_chassis = None
    
    # --- NEW: Resume Logic ---
    if check_for_resume_data():
        print("\nüîÑ Found previous session data!")
        user_input = input("Do you want to resume from previous session? (y/n): ").lower().strip()
        
        if user_input == 'y' or user_input == 'yes':
            print("üîÑ Resuming from previous session...")
            if load_resume_data():
                occupancy_map = create_occupancy_map_from_json()
                if occupancy_map is None:
                    print("‚ùå Failed to load occupancy map. Starting fresh session.")
                    occupancy_map = OccupancyGridMap(width=GRID, height=GRID)
                    RESUME_MODE = False
            else:
                print("‚ùå Failed to load resume data. Starting fresh session.")
                occupancy_map = OccupancyGridMap(width=GRID, height=GRID)
                RESUME_MODE = False
        else:
            print("üÜï Starting fresh session...")
            occupancy_map = OccupancyGridMap(width=GRID, height=GRID)
            RESUME_MODE = False
    else:
        print("üÜï No previous session found. Starting fresh session...")
        occupancy_map = OccupancyGridMap(width=GRID, height=GRID)
        RESUME_MODE = False
    
    # --- INTEGRATED OBJECT DETECTION SYSTEM ---
    print("üéØ Initializing Integrated Object Detection System...")
    
    # Initialize object detection components
    tracker = ObjectTracker(use_gpu=USE_GPU)
    manager = RMConnection()
    
    # ROI state (dynamic Y)
    roi_state = {"x": ROI_X0, "y": ROI_Y0, "w": ROI_W0, "h": ROI_H0}
    
    # Start object detection threads
    reconn = threading.Thread(target=reconnector_thread, args=(manager,), daemon=True)
    reconn.start()
    
    def is_detecting(): return is_detecting_flag["v"]
    
    cap_t = threading.Thread(target=capture_thread_func, args=(manager, frame_queue), daemon=True)
    proc_t = threading.Thread(target=processing_thread_func,
                              args=(tracker, frame_queue, TARGET_SHAPE, TARGET_COLOR, roi_state, is_detecting),
                              daemon=True)
    
    cap_t.start()
    proc_t.start()
    
    print("‚úÖ Object Detection System initialized (Camera ON, Detection OFF)")
    
    # Wait for camera to be ready - CRITICAL: Don't start exploration until camera is ready
    print("‚è≥ Waiting for camera to be ready...")
    camera_ready = False
    max_retries = 5
    
    for retry in range(max_retries):
        time.sleep(3.0)
        if manager.connected.is_set():
            print("‚úÖ Camera is ready!")
            camera_ready = True
            break
        else:
            print(f"‚ö†Ô∏è Camera not ready (attempt {retry + 1}/{max_retries}), retrying connection...")
            manager.connect()
            time.sleep(2.0)
    
    # Final check - CRITICAL: Don't proceed without camera
    if not camera_ready:
        print("‚ùå Camera connection failed after all retries. Please check camera connection and restart.")
        print("üõë Stopping program to prevent exploration without camera...")
        import sys
        sys.exit(1)
    
    print("üéØ Camera confirmed ready - Starting exploration...")
    
    # Start camera display thread (optional via SHOW_WINDOW flag)
    SHOW_WINDOW = True  # set False to disable display and reduce load on camera
    def camera_display_thread():
        print("üìπ Camera display thread started")
        display_frame = None
        frame_count = 0
        last_frame_time = time.time()

        try:
            while not stop_event.is_set():
                try:
                    # Get frame with shorter timeout to prevent blocking
                    display_frame = frame_queue.get(timeout=0.5)
                    frame_count += 1
                    last_frame_time = time.time()
                    
                    # Skip processing if frame is None or corrupted
                    if display_frame is None or display_frame.size == 0:
                        continue
                        
                except queue.Empty:
                    # Check if we're getting frames regularly
                    if time.time() - last_frame_time > 5.0 and frame_count > 0:
                        print("‚ö†Ô∏è No frames received for 5 seconds, checking camera connection...")
                        if not manager.connected.is_set():
                            print("üìπ Camera disconnected, attempting reconnect...")
                            manager.connect()
                    time.sleep(0.1)
                    continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Error getting frame: {e}")
                    time.sleep(0.1)
                    continue

                # ‡∏ß‡∏≤‡∏î ROI ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏ã‡πâ‡∏≤‡∏¢/‡∏Å‡∏•‡∏≤‡∏á/‡∏Ç‡∏ß‡∏≤
                ROI_X, ROI_Y, ROI_W, ROI_H = roi_state["x"], roi_state["y"], roi_state["w"], roi_state["h"]
                cv2.rectangle(display_frame, (ROI_X, ROI_Y), (ROI_X+ROI_W, ROI_Y+ROI_H), (255,0,0), 2)

                if is_detecting():
                    cv2.putText(display_frame, "MODE: DETECTING", (20,40),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
                    with output_lock:
                        details = processed_output["details"]

                    d1_abs = ROI_X + int(ROI_W*0.33)
                    d2_abs = ROI_X + int(ROI_W*0.66)
                    cv2.line(display_frame, (d1_abs, ROI_Y), (d1_abs, ROI_Y+ROI_H), (255,255,0), 1)
                    cv2.line(display_frame, (d2_abs, ROI_Y), (d2_abs, ROI_Y+ROI_H), (255,255,0), 1)

                    # ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏: ‡∏™‡∏µ/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
                    for det in details:
                        x,y,w,h = det['box']
                        abs_x, abs_y = x + ROI_X, y + ROI_Y
                        if det['is_target']:
                            box_color = (0,0,255)
                        elif det['shape'] == 'Uncertain':
                            box_color = (0,255,255)
                        else:
                            box_color = (0,255,0)
                        thickness = 4 if det['is_target'] else 2
                        cv2.rectangle(display_frame, (abs_x,abs_y), (abs_x+w, abs_y+h), box_color, thickness)
                        cv2.putText(display_frame, str(det['id']), (abs_x+5, abs_y+25),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, box_color, 2)

                    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≤‡∏¢‡∏ö‡∏ô
                    if details:
                        y_pos = 70
                        for obj in details:
                            target_str = " (TARGET!)" if obj['is_target'] else ""
                            line = f"ID {obj['id']}: {obj['color']} {obj['shape']}{target_str}"
                            # shadow
                            cv2.putText(display_frame, line, (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 4)
                            cv2.putText(display_frame, line, (20, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1)
                            y_pos += 25

                    # ‡∏ß‡∏≤‡∏î crosshair ‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡πâ‡∏ô bias +3¬∞
                    cy_bias = int(FRAME_H/2 - PITCH_BIAS_PIX)
                    cv2.line(display_frame, (FRAME_W//2 - 20, FRAME_H//2), (FRAME_W//2 + 20, FRAME_H//2), (255,255,255), 1)
                    cv2.line(display_frame, (FRAME_W//2, FRAME_H//2 - 20), (FRAME_W//2, FRAME_H//2 + 20), (255,255,255), 1)
                    cv2.line(display_frame, (0, cy_bias), (FRAME_W, cy_bias), (0, 128, 255), 1)  # ‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏õ‡πâ‡∏≤ +3¬∞

                else:
                    cv2.putText(display_frame, "MODE: VIEWING", (20,40),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,255), 2)

                # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ SDK
                st = "CONNECTED" if manager.connected.is_set() else "RECONNECTING..."
                cv2.putText(display_frame, f"SDK: {st}", (20, 70 if not is_detecting() else 45),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

                # Display frame with error handling
                try:
                    cv2.imshow("Robomaster Real-time Scan + PID Track (+3¬∞)", display_frame)
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q'):
                        print("üõë Quit key pressed, stopping display...")
                        break
                    elif key == ord('s'):
                        is_detecting_flag["v"] = not is_detecting_flag["v"]
                        print(f"üîç Detection {'ON' if is_detecting_flag['v'] else 'OFF'}")
                    elif key == ord('r'):
                        print("üîÑ Manual reconnect requested")
                        manager.drop_and_reconnect()
                except Exception as e:
                    print(f"‚ö†Ô∏è Error displaying frame: {e}")
                    time.sleep(0.1)
                    try:
                        while True: frame_queue.get_nowait()
                    except queue.Empty:
                        pass

        except Exception as e:
            print(f"‚ùå Camera display error: {e}")
        finally:
            try:
                cv2.destroyAllWindows()
            except Exception:
                pass
        print("üõë Camera display thread stopped")
    
    # Start camera display thread
    display_t = None
    if SHOW_WINDOW:
        display_t = threading.Thread(target=camera_display_thread, daemon=True)
        display_t.start()
    
    try:
        visualizer = RealTimeVisualizer(grid_size=GRID, target_dest=TARGET_DESTINATION)
        print("ü§ñ Connecting to robot...")
        ep_robot = robot.Robot()
        try:
            ep_robot.initialize(conn_type="ap")
            time.sleep(2.0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡πÉ‡∏´‡πâ robot initialize ‡πÄ‡∏™‡∏£‡πá‡∏à
        except Exception as e:
            print(f"‚ö†Ô∏è Robot connection error: {e}")
            print("üîÑ Retrying robot connection...")
            time.sleep(1.0)
            ep_robot.initialize(conn_type="ap")
            time.sleep(2.0)
        ep_chassis, ep_gimbal = ep_robot.chassis, ep_robot.gimbal
        ep_tof_sensor, ep_sensor_adaptor = ep_robot.sensor, ep_robot.sensor_adaptor
        
        print(" GIMBAL: Centering gimbal...")
        try:
            ep_gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed()
            time.sleep(0.5)  # Wait for gimbal to center
        except Exception as e:
            print(f"‚ö†Ô∏è Gimbal centering error: {e}")
            print("üîÑ Continuing without gimbal centering...")
        
        scanner = EnvironmentScanner(ep_sensor_adaptor, ep_tof_sensor, ep_gimbal, ep_chassis)
        movement_controller = MovementController(ep_chassis)
        attitude_handler.start_monitoring(ep_chassis)
        
        if RESUME_MODE:
            print("üîÑ Resuming exploration from previous position...")
            log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, "resume_session")
        else:
            print("üÜï Starting new exploration...")
            log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, "new_session_start")
        
        # --- INTEGRATED EXPLORATION WITH OBJECT DETECTION ---
        print("üöÄ Starting Integrated Exploration with Object Detection...")
        
        visited_cells = set()
        
        for step in range(40):  # max_steps
            try:
                r, c = CURRENT_POSITION
                print(f"\n--- Step {step + 1} at {CURRENT_POSITION}, Facing: {['N', 'E', 'S', 'W'][CURRENT_DIRECTION]} ---")
                
                log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, f"step_{step + 1}")
                
                print("   -> Resetting Yaw to ensure perfect alignment before new step...")
                attitude_handler.correct_yaw_to_target(ep_chassis, get_compensated_target_yaw())
                
                # Perform side alignment and mapping
                perform_side_alignment_and_mapping(movement_controller, scanner, attitude_handler, occupancy_map, visualizer)
                
                # --- AUTOMATIC OBJECT DETECTION AFTER ALIGNMENT ---
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥ object detection
                print("--- Performing Scan for Mapping (Front ToF Only) ---")
                is_front_occupied = scanner.get_front_tof_cm() < scanner.tof_wall_threshold_cm
                dir_map_abs_char = {0: 'N', 1: 'E', 2: 'S', 3: 'W'}
                occupancy_map.update_wall(r, c, dir_map_abs_char[CURRENT_DIRECTION], is_front_occupied, 'tof')
                
                # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏∏‡πà‡∏ô ‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∂‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡πà‡∏≠
                if not camera_is_healthy():
                    print("üõë Camera unhealthy ‚Üí pausing exploration and locking chassis until camera recovers...")
                    try:
                        movement_controller.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
                    except Exception:
                        pass
                    # wait loop with backoff
                    wait_start = time.time()
                    while not camera_is_healthy():
                        if time.time() - wait_start > 30.0:
                            print("‚ö†Ô∏è Camera recovery timeout (30s). Forcing reconnect and continuing wait...")
                            manager.drop_and_reconnect()
                            wait_start = time.time()
                        time.sleep(0.2)
                    print("‚úÖ Camera recovered. Resuming exploration...")
                
                # ‡πÉ‡∏ä‡πâ Gimbal Scan ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á 3 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á (‡∏ã‡πâ‡∏≤‡∏¢, ‡∏´‡∏ô‡πâ‡∏≤, ‡∏Ç‡∏ß‡∏≤)
                print("üîç Starting 3-direction gimbal scan after alignment...")
                if camera_is_healthy():
                    scan_all_directions_with_gimbal(scanner, occupancy_map)
                    print("üîç Gimbal scan completed")
                else:
                    print("üìπ Camera not ready - Skipping gimbal scan")
                
                # Check detection timer
                check_detection_timer()
                
                occupancy_map.update_node(r, c, False, 'tof')
                visited_cells.add((r, c))
                
                # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡πÅ‡∏°‡∏û‡∏ó‡∏∏‡∏Å 3 steps ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ thread
                if step % 3 == 0:
                    visualizer.update_plot(occupancy_map, CURRENT_POSITION)
                
                # Update IMU Drift Compensation
                nodes_visited = len(visited_cells)
                if nodes_visited >= IMU_COMPENSATION_START_NODE_COUNT:
                    compensation_intervals = nodes_visited // IMU_COMPENSATION_NODE_INTERVAL
                    new_compensation = compensation_intervals * IMU_COMPENSATION_DEG_PER_INTERVAL
                    if new_compensation != IMU_DRIFT_COMPENSATION_DEG:
                        IMU_DRIFT_COMPENSATION_DEG = new_compensation
                        print(f"üî© IMU Drift Compensation Updated: Visited {nodes_visited} nodes. New offset is {IMU_DRIFT_COMPENSATION_DEG:.1f}¬∞")
                
                # Continue with normal exploration logic
                priority_dirs = [(CURRENT_DIRECTION + 1) % 4, CURRENT_DIRECTION, (CURRENT_DIRECTION - 1 + 4) % 4]
                moved = False
                dir_vectors = [(-1, 0), (0, 1), (1, 0), (0, -1)]
                
                for target_dir in priority_dirs:
                    target_r, target_c = r + dir_vectors[target_dir][0], c + dir_vectors[target_dir][1]
                    
                    if occupancy_map.is_path_clear(r, c, target_r, target_c) and (target_r, target_c) not in visited_cells:
                        print(f"Path to {['N','E','S','W'][target_dir]} at ({target_r},{target_c}) seems clear. Attempting move.")
                        movement_controller.rotate_to_direction(target_dir, attitude_handler)
                        
                        print("    Ensuring gimbal is centered before ToF confirmation...")
                        t_start = time.time()
                        scanner.gimbal.moveto(pitch=0, yaw=0, yaw_speed=SPEED_ROTATE).wait_for_completed()
                        t_gimbal = time.time() - t_start
                        if t_gimbal > 2.0:
                            print(f"    ‚ö†Ô∏è Gimbal center took {t_gimbal:.2f}s (unusually long!)")
                        time.sleep(0.1)
                        
                        # Check camera health before ToF confirmation
                        if not camera_is_healthy(timeout=3.0):
                            print("üõë Camera unhealthy before path confirmation ‚Üí waiting for recovery...")
                            if not wait_for_camera_recovery(pause_label="Path Confirmation", max_wait=60.0):
                                print("‚ö†Ô∏è Camera recovery failed, but continuing with exploration...")
                        
                        print("    Confirming path forward with ToF...")
                        t_start = time.time()
                        is_blocked = scanner.get_front_tof_cm() < scanner.tof_wall_threshold_cm
                        t_tof = time.time() - t_start
                        if t_tof > 1.0:
                            print(f"    ‚ö†Ô∏è ToF read took {t_tof:.2f}s (unusually long!)")
                        
                        occupancy_map.update_wall(r, c, dir_map_abs_char[CURRENT_DIRECTION], is_blocked, 'tof')
                        print(f"    ToF confirmation: Wall belief updated. Path is {'BLOCKED' if is_blocked else 'CLEAR'}.")
                        visualizer.update_plot(occupancy_map, CURRENT_POSITION)
                        
                        # <<< NEW: Double-check with ToF after rotation >>>
                        if is_blocked:
                            print(f"    üö´ Wall detected! Turning back to original direction and recalculating path...")
                            movement_controller.rotate_to_direction(CURRENT_DIRECTION, attitude_handler)
                            print(f"    ‚úÖ Turned back to {['N','E','S','W'][CURRENT_DIRECTION]}. Re-evaluating available paths...")
                            continue  # Skip this direction and try next one
                        # <<< END OF NEW CODE >>>
                        
                        if occupancy_map.is_path_clear(r, c, target_r, target_c):
                            # Check camera health before moving
                            if not camera_is_healthy(timeout=3.0):
                                print("üõë Camera unhealthy before movement ‚Üí waiting for recovery...")
                                if not wait_for_camera_recovery(pause_label="Before Movement", max_wait=60.0):
                                    print("‚ö†Ô∏è Camera recovery failed, but continuing with movement...")
                            
                            axis_to_monitor = 'x' if ROBOT_FACE % 2 != 0 else 'y'
                            t_start = time.time()
                            movement_controller.move_forward_one_grid(axis=axis_to_monitor, attitude_handler=attitude_handler)
                            t_move = time.time() - t_start
                            if t_move > 15.0:
                                print(f"    ‚ö†Ô∏è Movement took {t_move:.2f}s (unusually long!)")
                            
                            movement_controller.center_in_node_with_tof(scanner, attitude_handler)
                            
                            CURRENT_POSITION = (target_r, target_c)
                            log_position_timestamp(CURRENT_POSITION, CURRENT_DIRECTION, "moved_to_new_node")
                            
                            # --- OBJECT DETECTION AFTER MOVING TO NEW POSITION ---
                            # ‡∏ñ‡πâ‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà ‡πÉ‡∏´‡πâ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
                            if not camera_is_healthy():
                                print("üõë Camera unhealthy after move ‚Üí locking chassis and waiting...")
                                try:
                                    movement_controller.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
                                except Exception:
                                    pass
                                wait_start = time.time()
                                while not camera_is_healthy():
                                    if time.time() - wait_start > 30.0:
                                        print("‚ö†Ô∏è Camera recovery timeout (30s). Forcing reconnect and continuing wait...")
                                        manager.drop_and_reconnect()
                                        wait_start = time.time()
                                    time.sleep(0.2)
                                print("‚úÖ Camera recovered. Continuing...")

                            # ‡πÉ‡∏ä‡πâ Gimbal Scan ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á 3 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
                            print("üîç Performing 3-direction gimbal scan at new position...")
                            if camera_is_healthy():
                                scan_all_directions_with_gimbal(scanner, occupancy_map)
                                print("üîç Gimbal scan completed at new position")
                            else:
                                print("üìπ Camera not ready - Skipping gimbal scan")
                            
                            moved = True
                            break
                        else:
                            print(f"    Confirmation failed. Path to {['N','E','S','W'][target_dir]} is blocked. Re-evaluating.")
                
                if not moved:
                    print("No immediate unvisited path. Initiating backtrack...")
                    backtrack_path = find_nearest_unvisited_path(occupancy_map, CURRENT_POSITION, visited_cells)
                    
                    if backtrack_path and len(backtrack_path) > 1:
                        execute_path(backtrack_path, movement_controller, attitude_handler, scanner, visualizer, occupancy_map)
                        print("Backtrack to new area complete. Resuming exploration.")
                        continue
                    else:
                        print("üéâ EXPLORATION COMPLETE! No reachable unvisited cells remain.")
                        break
            
            except Exception as e:
                print(f"\n‚ùå Error during step {step+1}: {e}")
                print("üõë Stopping robot and waiting for camera recovery...")
                try:
                    movement_controller.chassis.drive_wheels(w1=0, w2=0, w3=0, w4=0)
                except Exception:
                    pass
                if wait_for_camera_recovery(pause_label=f"Step {step+1} Recovery", max_wait=90.0):
                    print("‚úÖ Recovery complete. Resuming from current position...")
                else:
                    print("‚ö†Ô∏è Recovery failed. Resuming anyway from current position...")
                continue
        
        
        print("\nüéâ === INTEGRATED EXPLORATION PHASE FINISHED ===")
        
        print(f"\n\n--- NAVIGATION TO TARGET PHASE: From {CURRENT_POSITION} to {TARGET_DESTINATION} ---")
        
        if CURRENT_POSITION == TARGET_DESTINATION:
            print("üéâ Robot is already at the target destination!")
        else:
            path_to_target = find_path_bfs(occupancy_map, CURRENT_POSITION, TARGET_DESTINATION)
            if path_to_target and len(path_to_target) > 1:
                print(f"‚úÖ Path found to target: {path_to_target}")
                execute_path(path_to_target, movement_controller, attitude_handler, scanner, visualizer, occupancy_map, path_name="Final Navigation")
                print(f"üéâüéâ Robot has arrived at the target destination: {TARGET_DESTINATION}!")
            else:
                print(f"‚ö†Ô∏è Could not find a path from {CURRENT_POSITION} to {TARGET_DESTINATION}.")
        
    except KeyboardInterrupt: 
        print("\n‚ö†Ô∏è User interrupted exploration.")
        print("üíæ Saving data before exit...")
        if occupancy_map:
            save_all_data(occupancy_map)
    except Exception as e: 
        print(f"\n‚öå An error occurred: {e}")
        traceback.print_exc()
        print("üíæ Saving data before exit...")
        if occupancy_map:
            save_all_data(occupancy_map)
    finally:
        # Stop object detection threads
        stop_event.set()
        
        # Wait for threads to finish
        try:
            cap_t.join(timeout=2.0)
            proc_t.join(timeout=2.0)
            display_t.join(timeout=2.0)
        except Exception:
            pass
        
        # Close camera display
        try:
            cv2.destroyAllWindows()
        except Exception:
            pass
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏°‡πâ‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£ interrupt (‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô save_all_data)
        if occupancy_map:
            save_all_data(occupancy_map)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠
        if ep_robot:
            print("üîå Cleaning up and closing connection...")
            try:
                if scanner: scanner.cleanup()
                if attitude_handler and attitude_handler.is_monitoring: attitude_handler.stop_monitoring(ep_chassis)
                if movement_controller: movement_controller.cleanup()
                manager.close()
                ep_robot.close()
                print("üîå Connection closed.")
            except Exception as cleanup_error:
                print(f"‚ö†Ô∏è Error during cleanup: {cleanup_error}")
        
        print("... You can close the plot window now ...")
        plt.ioff()
        plt.show()